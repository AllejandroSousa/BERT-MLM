{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AllejandroSousa/BERT-MLM/blob/main/examples/nlp/ipynb/masked_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eGuloiGIF1q"
      },
      "source": [
        "# End-to-end Masked Language Modeling with BERT\n",
        "\n",
        "**Team:** [Allejandro Sousa](https://github.com/AllejandroSousa), [José Samuel](https://github.com/Samuel-IA7), [Vinícius Germano]()<br>\n",
        "**Date created:** 2025/04/03<br>\n",
        "**Last modified:** 2025/04/03<br>\n",
        "**Description:** \"Implement a Masked Language Model (MLM) using BERT and fine-tune it on the Stack Overflow Questions/Answers dataset. Additionally, implement a Markov chain for the same problem and compare the results.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3uB0kYZIF1s"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Masked Language Modeling is a fill-in-the-blank task,\n",
        "where a model uses the context words surrounding a mask token to try to predict what the\n",
        "masked word should be.\n",
        "\n",
        "For an input that contains one or more mask tokens,\n",
        "the model will generate the most likely substitution for each.\n",
        "\n",
        "Example:\n",
        "\n",
        "- Input: \"SyntaxError: Unexpected [mask].\"\n",
        "- Output: \"SyntaxError: Unexpected token.\"\n",
        "\n",
        "Masked language modeling is a great way to train a language\n",
        "model in a self-supervised setting (without human-annotated labels).\n",
        "Such a model can then be fine-tuned to accomplish various supervised\n",
        "NLP tasks.\n",
        "\n",
        "This example teaches you how to build a BERT model from scratch,\n",
        "train it with the masked language modeling task,\n",
        "and then fine-tune this model on a sentiment classification task.\n",
        "Additionally, we implement a Markov Chain-based approach to perform the same masked language modeling task and subsequent sentiment classification, allowing us to compare the results of these two distinct methods.\n",
        "\n",
        "We will use the Keras `TextVectorization` and `MultiHeadAttention` layers\n",
        "to create a BERT Transformer-Encoder network architecture, while the Markov Chain model leverages probabilistic transitions between words to predict masked tokens and generate features for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qXMeVLr1IF1t"
      },
      "outputs": [],
      "source": [
        "import os  # Importa a biblioteca 'os' para interagir com o sistema operacional, como definir variáveis de ambiente.\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"  # Define o backend do Keras como 'torch' (PyTorch). Isso determina qual biblioteca de baixo nível será usada para cálculos. Outras opções incluem 'jax' ou 'tensorflow'.\n",
        "\n",
        "import keras_hub  # Importa o Keras Hub, uma biblioteca que oferece camadas e modelos pré-construídos, especialmente úteis para tarefas de processamento de linguagem natural (NLP).\n",
        "\n",
        "import keras  # Importa o Keras, uma API de alto nível para criar e treinar modelos de deep learning de forma simplificada.\n",
        "from keras import layers  # Importa o módulo 'layers' do Keras, que contém componentes como camadas densas, embeddings, etc.\n",
        "from keras.layers import TextVectorization  # Importa especificamente a camada 'TextVectorization', usada para converter texto em sequências numéricas.\n",
        "\n",
        "from dataclasses import dataclass  # Importa o decorador 'dataclass' do Python, que facilita a criação de classes para armazenar dados.\n",
        "import pandas as pd  # Importa a biblioteca Pandas, amplamente usada para manipulação de dados em formato tabular.\n",
        "import numpy as np  # Importa o NumPy, essencial para operações numéricas rápidas e eficientes com arrays.\n",
        "import glob  # Importa a biblioteca 'glob' para encontrar arquivos no sistema usando padrões (ex.: '*.csv').\n",
        "import re  # Importa a biblioteca de expressões regulares ('re') para manipulação avançada de strings.\n",
        "from pprint import pprint  # Importa a função 'pprint' para exibir dados de forma estruturada e legível."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nYeiBrnIF1t"
      },
      "source": [
        "## Set-up Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SEWWu9wJIF1u"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"\n",
        "    Classe de configuração que define hiperparâmetros fixos para o modelo.\n",
        "\n",
        "    Atributos:\n",
        "        MAX_LEN (int): Comprimento máximo das sequências de texto após a vetorização.\n",
        "        BATCH_SIZE (int): Número de amostras processadas por lote durante o treinamento.\n",
        "        LR (float): Taxa de aprendizado (learning rate) usada pelo otimizador.\n",
        "        VOCAB_SIZE (int): Tamanho máximo do vocabulário para a camada de vetorização.\n",
        "        EMBED_DIM (int): Dimensão dos vetores de embedding gerados para cada palavra.\n",
        "        NUM_HEAD (int): Número de cabeças de atenção no modelo BERT, usado no mecanismo de atenção multi-cabeça.\n",
        "        FF_DIM (int): Dimensão da camada feed-forward interna do BERT.\n",
        "        NUM_LAYERS (int): Número de camadas de codificação no modelo BERT.\n",
        "    \"\"\"\n",
        "    MAX_LEN = 256  # Define o comprimento máximo das sequências como 256 tokens.\n",
        "    BATCH_SIZE = 32  # Define o tamanho do lote como 32 amostras.\n",
        "    LR = 0.001  # Define a taxa de aprendizado como 0.001 (um valor comum para iniciar).\n",
        "    VOCAB_SIZE = 30000  # Limita o vocabulário a 30.000 palavras mais frequentes.\n",
        "    EMBED_DIM = 128  # Define a dimensão dos embeddings como 128.\n",
        "    NUM_HEAD = 8  # Configura 8 cabeças de atenção para o BERT.\n",
        "    FF_DIM = 128  # Define a dimensão da camada feed-forward como 128.\n",
        "    NUM_LAYERS = 1  # Usa apenas 1 camada de codificação no BERT.\n",
        "\n",
        "config = Config()  # Cria uma instância da classe Config para acessar os hiperparâmetros ao longo do código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yAQh6s4IF1u"
      },
      "source": [
        "## Load the Data\n",
        "\n",
        "First, we will download the Stack Overflow dataset and load it into a Pandas DataFrame.\n",
        "\n",
        "### Important: Kaggle API Setup\n",
        "\n",
        "If you haven't uploaded the `kaggle.json` file to your Colab environment, follow these steps:\n",
        "\n",
        "1. Go to [Kaggle](https://www.kaggle.com).\n",
        "2. Log in to your account (or create one if you don’t have one).\n",
        "3. Navigate to **Settings**.\n",
        "4. Locate the **API** section and click on **Create New Token**.\n",
        "5. A `kaggle.json` file will be downloaded.\n",
        "6. Upload this file to your Colab environment before running the code below.\n",
        "\n",
        "Once the `kaggle.json` file is uploaded, proceed with the code execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "# Configurar o arquivo kaggle.json (após upload manual)\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Baixar o dataset\n",
        "!kaggle datasets download -d stackoverflow/stacksample\n",
        "\n",
        "# Descompactar\n",
        "!unzip stacksample.zip"
      ],
      "metadata": {
        "id": "BF5yrMx26IGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135770f1-baa0-4e6e-f042-79d5524bfeb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n",
            "Dataset URL: https://www.kaggle.com/datasets/stackoverflow/stacksample\n",
            "License(s): other\n",
            "stacksample.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  stacksample.zip\n",
            "replace Answers.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Carregar o dataset (assumindo que você baixou 'Questions.csv' do Kaggle)\n",
        "df = pd.read_csv(\"Questions.csv\", encoding='latin-1')\n",
        "\n",
        "# Filtrar para 200.000 exemplos e criar rótulos binários\n",
        "df = df.sample(n=200000, random_state=42)  # Reduzir para 200.000\n",
        "df[\"sentiment\"] = df[\"Score\"].apply(lambda x: 1 if x >= 1 else 0)  # 1 = positivo, 0 = negativo\n",
        "df[\"review\"] = df[\"Title\"] + \" \" + df[\"Body\"]  # Combinar título e corpo como texto principal\n",
        "\n",
        "# Dividir em treino e teste (50/50 como o IMDB)\n",
        "train_df, test_df = train_test_split(df, test_size=0.5, random_state=42)\n",
        "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "# Verificar o tamanho\n",
        "print(f\"Treino: {len(train_df)}, Teste: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "ypBefeET6QBB",
        "outputId": "7ccb3144-5289-4275-ca66-891e5177e9f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treino: 100000, Teste: 100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j28t9YhtIF1v"
      },
      "source": [
        "## Dataset preparation\n",
        "\n",
        "We will use the `TextVectorization` layer to vectorize the text into integer token ids.\n",
        "It transforms a batch of strings into either\n",
        "a sequence of token indices (one sample = 1D array of integer token indices, in order)\n",
        "or a dense representation (one sample = 1D array of float values encoding an unordered set of tokens).\n",
        "\n",
        "Below, we define 3 preprocessing functions.\n",
        "\n",
        "1.  The `get_vectorize_layer` function builds the `TextVectorization` layer.\n",
        "2.  The `encode` function encodes raw text into integer token ids.\n",
        "3.  The `get_masked_input_and_labels` function will mask input token ids.\n",
        "It masks 15% of all input tokens in each sequence at random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h6rqRj5mIF1v"
      },
      "outputs": [],
      "source": [
        "# For data pre-processing and tf.data.Dataset\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "    \"\"\"\n",
        "    Prepara o texto de entrada limpando e uniformizando ele para ser usado em um modelo.\n",
        "\n",
        "    Essa função faz uma limpeza no texto bruto para que ele fique em um formato mais simples e consistente antes de ser transformado em números que o modelo consegue entender. Ela:\n",
        "    - Transforma todas as letras em minúsculas para evitar diferenças entre \"Oi\" e \"oi\".\n",
        "    - Remove a tag HTML '<br />' (que significa quebra de linha) e coloca um espaço no lugar.\n",
        "    - Tira todos os sinais de pontuação e caracteres especiais, deixando só letras, números e espaços.\n",
        "\n",
        "    Args:\n",
        "        input_data (tf.Tensor): O texto bruto que será limpo. É um objeto do TensorFlow que guarda o texto.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: O texto limpo e uniformizado, pronto para ser processado.\n",
        "\n",
        "    Exemplo:\n",
        "        Entrada: \"Olá, Mundo! <br /> Como você está?\"\n",
        "        Saída: \"olá mundo como você está\"\n",
        "    \"\"\"\n",
        "    lowercase = tf.strings.lower(input_data)  # Transforma todo o texto em letras minúsculas para consistência.\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")  # Substitui a tag '<br />' por um espaço simples.\n",
        "    cleaned_text = tf.strings.regex_replace(\n",
        "        stripped_html,  # Usa o texto já sem '<br />'.\n",
        "        \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"),  # Define um padrão com todos os caracteres especiais a remover.\n",
        "        \"\"  # Substitui esses caracteres por nada (remove eles).\n",
        "    )\n",
        "    return cleaned_text  # Retorna o texto limpo e pronto para uso.\n",
        "\n",
        "# Links auxiliares:\n",
        "# - [tf.strings.lower](https://www.tensorflow.org/api_docs/python/tf/strings/lower): Como funciona a conversão para minúsculas.\n",
        "# - [tf.strings.regex_replace](https://www.tensorflow.org/api_docs/python/tf/strings/regex_replace): Explicação sobre substituição com expressões regulares.\n",
        "# - [re.escape](https://docs.python.org/3/library/re.html#re.escape): Como escapar caracteres especiais em padrões.\n",
        "\n",
        "\n",
        "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
        "    \"\"\"\n",
        "    Cria uma ferramenta que transforma textos em sequências de números para o modelo entender.\n",
        "\n",
        "    Essa função faz uma camada especial que pega textos (frases ou palavras) e transforma cada palavra em um número único, como um código. Ela também define um limite de palavras diferentes (vocabulário) e corta ou completa as sequências para terem o mesmo tamanho.\n",
        "\n",
        "    Args:\n",
        "        texts (list): Uma lista de frases ou textos que serão usados para \"ensinar\" quais palavras são importantes.\n",
        "        vocab_size (int): O número máximo de palavras diferentes que o modelo vai reconhecer.\n",
        "        max_seq (int): O tamanho fixo que cada sequência de números vai ter (corta se for maior, completa se for menor).\n",
        "        special_tokens (list, optional): Palavras especiais (como '[MASK]') que terão códigos próprios. Padrão é ['[MASK]'].\n",
        "\n",
        "    Returns:\n",
        "        layers.Layer: Uma camada configurada que pode transformar textos em números.\n",
        "\n",
        "    Exemplo:\n",
        "        texts = [\"olá mundo\", \"olá terra\"]\n",
        "        vocab_size = 5\n",
        "        max_seq = 3\n",
        "        Resultado: Uma camada que transforma \"olá mundo\" em algo como [2, 3, 0].\n",
        "    \"\"\"\n",
        "    vectorize_layer = TextVectorization(  # Cria a camada que vai transformar texto em números.\n",
        "        max_tokens=vocab_size,  # Define o limite de palavras diferentes que ela vai aprender.\n",
        "        output_mode=\"int\",  # Faz a saída ser números inteiros (cada número é uma palavra).\n",
        "        standardize=custom_standardization,  # Usa a função de limpeza que definimos antes.\n",
        "        output_sequence_length=max_seq,  # Garante que todas as sequências tenham esse tamanho fixo.\n",
        "    )\n",
        "    vectorize_layer.adapt(texts)  # \"Treina\" a camada com os textos para ela aprender as palavras mais comuns.\n",
        "    vocab = vectorize_layer.get_vocabulary()  # Pega a lista de palavras que a camada aprendeu.\n",
        "    vocab = vocab[2:vocab_size - len(special_tokens)] + [\"[mask]\"]  # Tira as palavras padrão '[PAD]' e '[UNK]' e adiciona '[mask]'.\n",
        "    vectorize_layer.set_vocabulary(vocab)  # Atualiza a camada com o novo vocabulário que inclui '[mask]'.\n",
        "    return vectorize_layer  # Retorna a camada pronta para transformar textos em números.\n",
        "\n",
        "# Links auxiliares:\n",
        "# - [TextVectorization](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/): Documentação oficial da camada.\n",
        "# - [Método adapt](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/#adapt-method): Como a camada aprende o vocabulário.\n",
        "\n",
        "\n",
        "vectorize_layer = get_vectorize_layer(  # Cria uma camada de vetorização chamando a função 'get_vectorize_layer' (assumida como definida em outro lugar).\n",
        "    all_data.review.values.tolist(),  # Passa a lista de textos da coluna 'review' do DataFrame 'all_data' para adaptar o vetorizador ao vocabulário.\n",
        "    config.VOCAB_SIZE,  # Define o tamanho máximo do vocabulário como especificado em 'config' (30.000 palavras).\n",
        "    config.MAX_LEN,  # Define o comprimento máximo das sequências como especificado em 'config' (256 tokens).\n",
        "    special_tokens=[\"[mask]\"],  # Inclui o token especial '[mask]' no vocabulário, usado para tarefas de masked language modeling (MLM).\n",
        ")\n",
        "\n",
        "# Obtém o ID numérico do token '[mask]'. A camada de vetorização converte '[mask]' em um número, que é extraído como um array NumPy e acessado na posição [0][0].\n",
        "mask_token_id = vectorize_layer([\"[mask]\"]).cpu().numpy()[0][0]\n",
        "\n",
        "def encode(texts):\n",
        "    \"\"\"\n",
        "    Transforma uma lista de textos em uma lista de números usando a camada de vetorização.\n",
        "\n",
        "    Essa função pega textos brutos e usa a camada que criamos antes para transformar cada palavra em um número, criando uma matriz de números que o modelo pode usar.\n",
        "\n",
        "    Args:\n",
        "        texts (list): Lista de frases ou textos para transformar em números.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Uma matriz de números (array do NumPy) onde cada linha é um texto transformado.\n",
        "\n",
        "    Exemplo:\n",
        "        texts = [\"olá mundo\"]\n",
        "        Resultado: [[2, 3]] (supondo que 'olá' é 2 e 'mundo' é 3).\n",
        "    \"\"\"\n",
        "    encoded_texts = vectorize_layer(texts)  # Usa a camada de vetorização para transformar os textos em números.\n",
        "    return encoded_texts.cpu().numpy()  # Converte o resultado de um objeto TensorFlow para um array comum do NumPy.\n",
        "\n",
        "# Links auxiliares:\n",
        "# - [NumPy arrays](https://numpy.org/doc/stable/user/absolute_beginners.html): Introdução aos arrays do NumPy.\n",
        "\n",
        "\n",
        "def get_masked_input_and_labels(encoded_texts):\n",
        "    \"\"\"\n",
        "    Prepara os textos numerados para um jogo de adivinhação onde algumas palavras são escondidas.\n",
        "\n",
        "    Essa função cria uma versão dos textos onde algumas palavras são substituídas por '[MASK]' ou outras palavras aleatórias. É como um treino para o modelo aprender a adivinhar palavras faltando.\n",
        "\n",
        "    Args:\n",
        "        encoded_texts (np.ndarray): Matriz de números que representam os textos já transformados.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Três coisas:\n",
        "            - encoded_texts_masked (np.ndarray): Textos com algumas palavras escondidas ou trocadas.\n",
        "            - y_labels (np.ndarray): As palavras originais (o que o modelo deve adivinhar).\n",
        "            - sample_weights (np.ndarray): Pesos que dizem quais palavras importam no treino.\n",
        "\n",
        "    Exemplo:\n",
        "        encoded_texts = [[2, 3, 4]] (olá mundo terra)\n",
        "        Resultado: ([2, 4999, 4], [2, 3, 4], [1, 1, 1]) onde 4999 é '[MASK]'.\n",
        "    \"\"\"\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15  # Cria uma matriz de True/False escolhendo 15% dos números aleatoriamente.\n",
        "    inp_mask[encoded_texts <= 2] = False  # Não esconde números especiais como 0 ('[PAD]') ou 1 ('[UNK]').\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)  # Cria uma matriz cheia de -1 (significa \"ignorar\").\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]  # Coloca os números originais onde vamos esconder (para o modelo adivinhar depois).\n",
        "    encoded_texts_masked = np.copy(encoded_texts)  # Faz uma cópia dos textos numerados para modificar.\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)  # Escolhe 90% dos escondidos para virar '[MASK]'.\n",
        "    encoded_texts_masked[inp_mask_2mask] = mask_token_id  # Substitui esses 90% pelo número que representa '[MASK]'.\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)  # Escolhe 10% dos '[MASK]' para virar aleatórios.\n",
        "    random_tokens = np.random.randint(3, mask_token_id, inp_mask_2random.sum())  # Gera números aleatórios para substituir.\n",
        "    encoded_texts_masked[inp_mask_2random] = random_tokens  # Coloca esses números aleatórios nos lugares escolhidos.\n",
        "    sample_weights = np.ones(labels.shape)  # Cria uma matriz de 1s (todos os números importam por padrão).\n",
        "    sample_weights[labels == -1] = 0  # Muda para 0 onde não há nada para adivinhar (labels = -1).\n",
        "    y_labels = np.copy(encoded_texts)  # Faz uma cópia dos textos originais como as respostas corretas.\n",
        "    return encoded_texts_masked, y_labels, sample_weights  # Retorna os textos modificados, as respostas e os pesos.\n",
        "\n",
        "# Links auxiliares:\n",
        "# - [np.random.rand](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html): Como gerar números aleatórios.\n",
        "# - [BERT Masking Strategy](https://huggingface.co/docs/transformers/model_doc/bert#bertformaskedlm): Explicação sobre o método de esconder palavras.\n",
        "\n",
        "\n",
        "x_train = encode(train_df.review.values)  # Codifica os textos da coluna 'review' do conjunto de treino usando a função 'encode' (assumida como definida), que aplica o vetorizador.\n",
        "y_train = train_df.sentiment.values  # Extrai os rótulos de sentimento do conjunto de treino como um array.\n",
        "train_classifier_ds = (  # Cria um dataset TensorFlow para o treinamento do classificador.\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))  # Converte os arrays 'x_train' e 'y_train' em um dataset, dividindo-os em pares (entrada, rótulo).\n",
        "    .shuffle(1000)  # Embaralha os dados com um buffer de 1000 amostras para introduzir aleatoriedade no treinamento.\n",
        "    .batch(config.BATCH_SIZE)  # Agrupa os dados em lotes de tamanho 32 (definido em 'config.BATCH_SIZE').\n",
        ")\n",
        "\n",
        "x_test = encode(test_df.review.values)  # Codifica os textos da coluna 'review' do conjunto de teste.\n",
        "y_test = test_df.sentiment.values  # Extrai os rótulos de sentimento do conjunto de teste.\n",
        "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
        "    config.BATCH_SIZE\n",
        ")  # Cria um dataset de teste a partir de 'x_test' e 'y_test', agrupando em lotes de tamanho 32.\n",
        "\n",
        "test_raw_classifier_ds = test_df  # Armazena o DataFrame bruto de teste ('test_df') para uso posterior em um modelo end-to-end, sem codificação prévia.\n",
        "\n",
        "x_all_review = encode(all_data.review.values)  # Codifica todos os textos da coluna 'review' de 'all_data' para treinar o modelo de linguagem mascarada (MLM).\n",
        "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
        "    x_all_review\n",
        ")  # Gera dados para o MLM: 'x_masked_train' (textos com tokens mascarados), 'y_masked_labels' (rótulos dos tokens mascarados), e 'sample_weights' (pesos para a perda).\n",
        "\n",
        "mlm_ds = tf.data.Dataset.from_tensor_slices(  # Cria um dataset TensorFlow para o treinamento do MLM.\n",
        "    (x_masked_train, y_masked_labels, sample_weights)  # Usa os três arrays gerados para formar tuplas de dados.\n",
        ")\n",
        "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)  # Embaralha o dataset com um buffer de 1000 e agrupa em lotes de tamanho 32."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S4msCaxIF1v"
      },
      "source": [
        "## Create BERT model (Pretraining Model) for masked language modeling\n",
        "\n",
        "We will create a BERT-like pretraining model architecture\n",
        "using the `MultiHeadAttention` layer.\n",
        "It will take token ids as inputs (including masked tokens)\n",
        "and it will predict the correct ids for the masked input tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8fCXsRzqIF1w",
        "outputId": "82c0b509-e468-400e-a49c-a1819dcc36f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"masked_bert_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"masked_bert_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ word_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m3,840,000\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ position_embedding… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m32,768\u001b[0m │ word_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mPositionEmbedding\u001b[0m) │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ word_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │                   │            │ position_embeddi… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_0_multihea… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m66,048\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_0_att_drop… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ encoder_0_multih… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ encoder_0_att_dr… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_0_att_laye… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_0_ffn       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m33,024\u001b[0m │ encoder_0_att_la… │\n",
              "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_0_ffn_drop… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ encoder_0_ffn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ encoder_0_att_la… │\n",
              "│                     │                   │            │ encoder_0_ffn_dr… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_0_ffn_laye… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mlm_cls (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m,       │  \u001b[38;5;34m3,870,000\u001b[0m │ encoder_0_ffn_la… │\n",
              "│                     │ \u001b[38;5;34m30000\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ word_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ position_embedding… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEmbedding</span>) │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │                   │            │ position_embeddi… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_0_multihea… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_0_att_drop… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_0_multih… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ encoder_0_att_dr… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_0_att_laye… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_0_ffn       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ encoder_0_att_la… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_0_ffn_drop… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_0_ffn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_0_att_la… │\n",
              "│                     │                   │            │ encoder_0_ffn_dr… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_0_ffn_laye… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mlm_cls (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,870,000</span> │ encoder_0_ffn_la… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,842,352\u001b[0m (29.92 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,842,352</span> (29.92 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,842,352\u001b[0m (29.92 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,842,352</span> (29.92 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "def bert_module(query, key, value, i):\n",
        "    \"\"\"\n",
        "    Constrói um pedaço do modelo que presta atenção em partes importantes do texto.\n",
        "\n",
        "    Essa função cria uma parte do modelo que olha para todas as palavras ao mesmo tempo e decide quais são mais importantes, depois ajusta os números para ficarem melhores.\n",
        "\n",
        "    Args:\n",
        "        query (tf.Tensor): Números que representam o texto que estamos analisando.\n",
        "        key (tf.Tensor): Números que ajudam a decidir o que é importante.\n",
        "        value (tf.Tensor): Números que serão ajustados com base na importância.\n",
        "        i (int): Um número para identificar essa parte do modelo (se houver várias).\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Os números ajustados depois de prestar atenção e refinar.\n",
        "\n",
        "    Exemplo:\n",
        "        query = key = value = [[1, 2], [3, 4]]\n",
        "        Resultado: Um tensor ajustado com base na atenção entre os números.\n",
        "    \"\"\"\n",
        "    attention_output = layers.MultiHeadAttention(  # Cria uma camada que presta atenção em várias partes do texto ao mesmo tempo.\n",
        "        num_heads=config.NUM_HEAD,  # Quantas \"cabeças\" vão olhar o texto (mais cabeças = mais detalhes).\n",
        "        key_dim=config.EMBED_DIM // config.NUM_HEAD,  # Tamanho de cada pedaço de informação que cada cabeça vê.\n",
        "        name=f\"encoder_{i}_multiheadattention\"  # Nome único para essa camada.\n",
        "    )(query, key, value)  # Faz as cabeças olharem para query, key e value e decidirem o que importa.\n",
        "    attention_output = layers.Dropout(0.1, name=f\"encoder_{i}_att_dropout\")(attention_output)  # Descarta 10% dos dados aleatoriamente para evitar depender demais de alguns números.\n",
        "    attention_output = layers.LayerNormalization(  # Ajusta os números para ficarem mais consistentes (média 0, variação 1).\n",
        "        epsilon=1e-6,  # Um número pequeno para evitar divisão por zero.\n",
        "        name=f\"encoder_{i}_att_layernormalization\"  # Nome único para essa camada.\n",
        "    )(query + attention_output)  # Combina os números originais com os ajustados (conexão residual).\n",
        "    ffn = keras.Sequential(  # Cria uma mini-rede com duas camadas para refinar os números ainda mais.\n",
        "        [\n",
        "            layers.Dense(config.FF_DIM, activation=\"relu\"),  # Primeira camada: aumenta os detalhes e usa ReLU para zerar números negativos.\n",
        "            layers.Dense(config.EMBED_DIM)  # Segunda camada: volta ao tamanho original sem ativação especial.\n",
        "        ],\n",
        "        name=f\"encoder_{i}_ffn\"  # Nome único para essa mini-rede.\n",
        "    )\n",
        "    ffn_output = ffn(attention_output)  # Passa os números ajustados pela mini-rede.\n",
        "    ffn_output = layers.Dropout(0.1, name=f\"encoder_{i}_ffn_dropout\")(ffn_output)  # Descarta 10% dos dados novamente.\n",
        "    sequence_output = layers.LayerNormalization(  # Ajusta os números mais uma vez para consistência.\n",
        "        epsilon=1e-6,  # Pequeno número para evitar problemas matemáticos.\n",
        "        name=f\"encoder_{i}_ffn_layernormalization\"  # Nome único para essa camada.\n",
        "    )(attention_output + ffn_output)  # Combina os números da atenção com os da mini-rede.\n",
        "    return sequence_output  # Retorna os números finais dessa parte do modelo.\n",
        "\n",
        "# Links auxiliares:\n",
        "# - [MultiHeadAttention](https://keras.io/api/layers/attention_layers/multi_head_attention/): Como funciona a atenção com várias cabeças.\n",
        "# - [LayerNormalization](https://keras.io/api/layers/normalization_layers/layer_normalization/): Explicação sobre normalização.\n",
        "# - [Dropout](https://keras.io/api/layers/regularization_layers/dropout/): Por que descartar dados ajuda o modelo.\n",
        "\n",
        "# Define a função de perda como entropia cruzada categórica esparsa, sem redução automática (ou seja, retorna a perda por amostra, útil para o MLM).\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(reduction=None)\n",
        "# Cria um objeto para rastrear a média da perda durante o treinamento, nomeado como 'loss'.\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "class MaskedLanguageModel(keras.Model):\n",
        "    \"\"\"\n",
        "    Um modelo especial que aprende a adivinhar palavras escondidas nos textos.\n",
        "\n",
        "    Essa classe define como o modelo calcula o erro (perda) e acompanha o progresso enquanto treina para adivinhar palavras que foram escondidas.\n",
        "\n",
        "    Exemplo:\n",
        "        O modelo pega \"olá [MASK] terra\" e tenta adivinhar que '[MASK]' é 'mundo'.\n",
        "    \"\"\"\n",
        "    def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Calcula o quanto o modelo está errando nas suas previsões.\n",
        "\n",
        "        Args:\n",
        "            x: Entrada (não usada aqui diretamente).\n",
        "            y (tf.Tensor): As palavras corretas que o modelo deveria prever.\n",
        "            y_pred (tf.Tensor): As previsões do modelo.\n",
        "            sample_weight (tf.Tensor): Pesos que dizem quais previsões importam mais.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: O erro total calculado.\n",
        "        \"\"\"\n",
        "        loss = loss_fn(y, y_pred, sample_weight)  # Calcula o erro comparando as previsões com as respostas certas.\n",
        "        loss_tracker.update_state(loss, sample_weight=sample_weight)  # Atualiza um contador que acompanha o erro médio.\n",
        "        return keras.ops.sum(loss)  # Soma todo o erro para dar um número único.\n",
        "\n",
        "    def compute_metrics(self, x, y, y_pred, sample_weight):\n",
        "        \"\"\"\n",
        "        Mostra como o modelo está se saindo.\n",
        "\n",
        "        Args:\n",
        "            x, y, y_pred, sample_weight: Mesmos significados que em compute_loss.\n",
        "\n",
        "        Returns:\n",
        "            dict: Um dicionário com o erro médio até agora.\n",
        "        \"\"\"\n",
        "        return {\"loss\": loss_tracker.result()}  # Retorna o erro médio guardado no contador.\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        \"\"\"\n",
        "        Diz quais números o modelo vai acompanhar enquanto treina.\n",
        "\n",
        "        Returns:\n",
        "            list: Lista com o contador de erro.\n",
        "        \"\"\"\n",
        "        return [loss_tracker]  # Inclui o contador de erro para ser mostrado durante o treino.\n",
        "\n",
        "# Links auxiliares:\n",
        "# - [Criando modelos personalizados](https://keras.io/guides/making_new_layers_and_models_via_subclassing/): Como fazer classes como essa.\n",
        "\n",
        "\n",
        "def create_masked_language_bert_model():\n",
        "    \"\"\"\n",
        "    Monta um modelo completo para adivinhar palavras escondidas nos textos.\n",
        "\n",
        "    Esse modelo junta várias partes: transforma números em informações detalhadas, presta atenção nas palavras importantes e tenta prever as que estão faltando.\n",
        "\n",
        "    Returns:\n",
        "        MaskedLanguageModel: O modelo pronto para treinar.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=\"int64\")  # Cria a entrada para sequências de números (textos transformados).\n",
        "    word_embeddings = layers.Embedding(  # Transforma cada número em um vetor de informações mais rico.\n",
        "        config.VOCAB_SIZE,  # Número total de palavras que o modelo conhece.\n",
        "        config.EMBED_DIM,  # Tamanho de cada vetor de informações.\n",
        "        name=\"word_embedding\"  # Nome dessa camada.\n",
        "    )(inputs)  # Aplica essa transformação nos números de entrada.\n",
        "    position_embeddings = keras_hub.layers.PositionEmbedding(  # Adiciona informações sobre a posição de cada palavra na frase.\n",
        "        sequence_length=config.MAX_LEN  # Tamanho máximo da frase.\n",
        "    )(word_embeddings)  # Aplica isso nos vetores de palavras.\n",
        "    embeddings = word_embeddings + position_embeddings  # Combina as informações das palavras com suas posições.\n",
        "    encoder_output = embeddings  # Começa com os vetores combinados.\n",
        "    for i in range(config.NUM_LAYERS):  # Repete várias vezes para refinar os dados.\n",
        "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)  # Passa por uma parte que presta atenção e refina.\n",
        "    mlm_output = layers.Dense(  # Cria uma camada final para adivinhar as palavras.\n",
        "        config.VOCAB_SIZE,  # Número de palavras possíveis para prever.\n",
        "        name=\"mlm_cls\",  # Nome dessa camada.\n",
        "        activation=\"softmax\"  # Transforma os resultados em probabilidades (soma 1).\n",
        "    )(encoder_output)  # Faz a previsão com base nos dados refinados.\n",
        "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")  # Junta tudo em um modelo especial.\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)  # Define como o modelo vai aprender ajustando os erros.\n",
        "    mlm_model.compile(optimizer=optimizer)  # Prepara o modelo para ser treinado.\n",
        "    return mlm_model  # Retorna o modelo pronto.\n",
        "\n",
        "# Links auxiliares:\n",
        "# - [Embedding Layer](https://keras.io/api/layers/core_layers/embedding/): Como transformar números em vetores.\n",
        "# - [PositionEmbedding](https://keras.io/api/keras_nlp/layers/position_embedding/): Sobre adicionar posição às palavras.\n",
        "# - [Adam Optimizer](https://keras.io/api/optimizers/adam/): Como o modelo ajusta os erros.\n",
        "\n",
        "# Cria um dicionário que mapeia IDs (índices) para tokens (palavras) com base no vocabulário do vetorizador.\n",
        "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
        "# Inverte o dicionário, criando um mapeamento de tokens para IDs.\n",
        "token2id = {y: x for x, y in id2token.items()}\n",
        "\n",
        "\n",
        "class MaskedTextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Uma ferramenta que mostra como o modelo está adivinhando palavras escondidas durante o treino.\n",
        "\n",
        "    Essa classe acompanha o treino e, a cada etapa, testa o modelo com um texto exemplo para ver o que ele prevê no lugar de '[MASK]'.\n",
        "\n",
        "    Args:\n",
        "        sample_tokens (np.ndarray): Um texto numerado com '[MASK]' para testar.\n",
        "        top_k (int): Quantas previsões principais mostrar (padrão é 5).\n",
        "    \"\"\"\n",
        "    def __init__(self, sample_tokens, top_k=5):\n",
        "        super().__init__()  # Configura a classe base (necessário para callbacks).\n",
        "        self.sample_tokens = sample_tokens  # Guarda o texto numerado de exemplo.\n",
        "        self.k = top_k  # Define quantas previsões principais serão exibidas.\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        \"\"\"\n",
        "        Transforma uma sequência de números de volta em texto legível.\n",
        "\n",
        "        Args:\n",
        "            tokens (np.ndarray): Lista de números representando palavras.\n",
        "\n",
        "        Returns:\n",
        "            str: O texto reconstruído.\n",
        "        \"\"\"\n",
        "        return \" \".join([id2token[t] for t in tokens if t != 0])  # Junta as palavras correspondentes aos números, ignorando 0 ('[PAD]').\n",
        "\n",
        "    def convert_ids_to_tokens(self, id):\n",
        "        \"\"\"\n",
        "        Transforma um único número em sua palavra correspondente.\n",
        "\n",
        "        Args:\n",
        "            id (int): O número a converter.\n",
        "\n",
        "        Returns:\n",
        "            str: A palavra associada ao número.\n",
        "        \"\"\"\n",
        "        return id2token[id]  # Pega a palavra do dicionário id2token.\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\"\n",
        "        Executa no final de cada etapa de treino para mostrar as previsões.\n",
        "\n",
        "        Args:\n",
        "            epoch (int): Número da etapa atual.\n",
        "            logs (dict): Informações do treino (não usadas aqui).\n",
        "        \"\"\"\n",
        "        prediction = self.model.predict(self.sample_tokens)  # Usa o modelo para prever as palavras no texto exemplo.\n",
        "        masked_index = np.where(self.sample_tokens == mask_token_id)[1]  # Encontra onde está o '[MASK]' no texto.\n",
        "        mask_prediction = prediction[0][masked_index]  # Pega as probabilidades previstas para o '[MASK]'.\n",
        "        top_indices = mask_prediction[0].argsort()[-self.k:][::-1]  # Escolhe os 5 números com maiores probabilidades (ordem decrescente).\n",
        "        values = mask_prediction[0][top_indices]  # Pega as probabilidades correspondentes a esses números.\n",
        "        for i in range(len(top_indices)):  # Para cada uma das top previsões:\n",
        "            p = top_indices[i]  # Pega o número previsto.\n",
        "            v = values[i]  # Pega a probabilidade associada.\n",
        "            tokens = np.copy(sample_tokens[0])  # Faz uma cópia do texto exemplo.\n",
        "            tokens[masked_index[0]] = p  # Substitui '[MASK]' pela previsão.\n",
        "            result = {  # Cria um dicionário com os resultados.\n",
        "                \"input_text\": self.decode(sample_tokens[0].cpu().numpy()),  # Texto original com '[MASK]'.\n",
        "                \"prediction\": self.decode(tokens),  # Texto com a previsão no lugar de '[MASK]'.\n",
        "                \"probability\": v,  # Probabilidade dessa previsão.\n",
        "                \"predicted mask token\": self.convert_ids_to_tokens(p),  # Palavra prevista sozinha.\n",
        "            }\n",
        "            pprint(result)  # Mostra o resultado de forma organizada.\n",
        "\n",
        "# Links auxiliares:\n",
        "# - [Callbacks no Keras](https://keras.io/api/callbacks/): Como criar ferramentas que acompanham o treino.\n",
        "# - [np.where](https://numpy.org/doc/stable/reference/generated/numpy.where.html): Como encontrar posições em arrays.\n",
        "\n",
        "\n",
        "# Vetoriza o texto de exemplo \"SyntaxError: Unexpected [mask]\", convertendo-o em uma sequência numérica.\n",
        "sample_tokens = vectorize_layer([\"SyntaxError: Unexpected [mask]\"])\n",
        "# Instancia um callback personalizado 'MaskedTextGenerator' (assumido como definido) com a sequência numérica, para gerar texto durante o treinamento.\n",
        "generator_callback = MaskedTextGenerator(sample_tokens.cpu().numpy())\n",
        "\n",
        "# Cria um modelo BERT para masked language modeling chamando a função 'create_masked_language_bert_model' (assumida como definida).\n",
        "bert_masked_model = create_masked_language_bert_model()\n",
        "bert_masked_model.summary()  # Exibe um resumo da arquitetura do modelo, como número de camadas e parâmetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuoDrtsFIF1w"
      },
      "source": [
        "## Train and Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LO76aubFIF1w",
        "outputId": "7f57b68c-33f4-4d1d-a834-f3152af78c49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('error'),\n",
            " 'prediction': 'syntaxerror unexpected error',\n",
            " 'probability': np.float32(0.098302424)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('in'),\n",
            " 'prediction': 'syntaxerror unexpected in',\n",
            " 'probability': np.float32(0.041935407)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('not'),\n",
            " 'prediction': 'syntaxerror unexpected not',\n",
            " 'probability': np.float32(0.022850646)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('to'),\n",
            " 'prediction': 'syntaxerror unexpected to',\n",
            " 'probability': np.float32(0.022426281)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('with'),\n",
            " 'prediction': 'syntaxerror unexpected with',\n",
            " 'probability': np.float32(0.021635888)}\n",
            "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1905s\u001b[0m 305ms/step - loss: 6.7274\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('error'),\n",
            " 'prediction': 'syntaxerror unexpected error',\n",
            " 'probability': np.float32(0.12600833)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('issue'),\n",
            " 'prediction': 'syntaxerror unexpected issue',\n",
            " 'probability': np.float32(0.024543114)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('unexpected'),\n",
            " 'prediction': 'syntaxerror unexpected unexpected',\n",
            " 'probability': np.float32(0.01776239)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('exception'),\n",
            " 'prediction': 'syntaxerror unexpected exception',\n",
            " 'probability': np.float32(0.015116588)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('errors'),\n",
            " 'prediction': 'syntaxerror unexpected errors',\n",
            " 'probability': np.float32(0.014537058)}\n",
            "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1901s\u001b[0m 304ms/step - loss: 5.3347\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('error'),\n",
            " 'prediction': 'syntaxerror unexpected error',\n",
            " 'probability': np.float32(0.39354366)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('unexpected'),\n",
            " 'prediction': 'syntaxerror unexpected unexpected',\n",
            " 'probability': np.float32(0.10184394)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('syntax'),\n",
            " 'prediction': 'syntaxerror unexpected syntax',\n",
            " 'probability': np.float32(0.040396504)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('errors'),\n",
            " 'prediction': 'syntaxerror unexpected errors',\n",
            " 'probability': np.float32(0.0179325)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('issue'),\n",
            " 'prediction': 'syntaxerror unexpected issue',\n",
            " 'probability': np.float32(0.016566578)}\n",
            "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1900s\u001b[0m 304ms/step - loss: 4.4386\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('error'),\n",
            " 'prediction': 'syntaxerror unexpected error',\n",
            " 'probability': np.float32(0.2372819)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('issue'),\n",
            " 'prediction': 'syntaxerror unexpected issue',\n",
            " 'probability': np.float32(0.0917664)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('unexpected'),\n",
            " 'prediction': 'syntaxerror unexpected unexpected',\n",
            " 'probability': np.float32(0.04790539)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('syntax'),\n",
            " 'prediction': 'syntaxerror unexpected syntax',\n",
            " 'probability': np.float32(0.04259776)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('errors'),\n",
            " 'prediction': 'syntaxerror unexpected errors',\n",
            " 'probability': np.float32(0.029050961)}\n",
            "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1901s\u001b[0m 304ms/step - loss: 3.9852\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('error'),\n",
            " 'prediction': 'syntaxerror unexpected error',\n",
            " 'probability': np.float32(0.22705351)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('syntax'),\n",
            " 'prediction': 'syntaxerror unexpected syntax',\n",
            " 'probability': np.float32(0.07655566)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('issue'),\n",
            " 'prediction': 'syntaxerror unexpected issue',\n",
            " 'probability': np.float32(0.068694495)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('unexpected'),\n",
            " 'prediction': 'syntaxerror unexpected unexpected',\n",
            " 'probability': np.float32(0.052827355)}\n",
            "{'input_text': 'syntaxerror unexpected [mask]',\n",
            " 'predicted mask token': np.str_('errors'),\n",
            " 'prediction': 'syntaxerror unexpected errors',\n",
            " 'probability': np.float32(0.029907692)}\n",
            "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1899s\u001b[0m 304ms/step - loss: 3.7363\n"
          ]
        }
      ],
      "source": [
        "# Treina o modelo no dataset 'mlm_ds' por 5 épocas, usando o callback 'generator_callback' para monitorar ou gerar previsões.\n",
        "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
        "bert_masked_model.save(\"bert_mlm_stackoverflow.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rnBmQPFIF1w"
      },
      "source": [
        "## Fine-tune a sentiment classification model\n",
        "\n",
        "We will fine-tune our self-supervised model on a downstream task of sentiment classification.\n",
        "To do this, let's create a classifier by adding a pooling layer and a `Dense` layer on top of the\n",
        "pretrained BERT features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CqKAc2iCIF1w",
        "outputId": "b5f6c0f9-e64f-4fd7-fe47-c263df52bbac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"classification\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"classification\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ functional_4 (\u001b[38;5;33mFunctional\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m3,972,352\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ functional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,972,352</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,980,673\u001b[0m (15.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,980,673</span> (15.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,321\u001b[0m (32.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,321</span> (32.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,972,352\u001b[0m (15.15 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,972,352</span> (15.15 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 27ms/step - accuracy: 0.5385 - loss: 0.7281 - val_accuracy: 0.5487 - val_loss: 0.6957\n",
            "Epoch 2/5\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 26ms/step - accuracy: 0.5617 - loss: 0.6832 - val_accuracy: 0.5693 - val_loss: 0.6783\n",
            "Epoch 3/5\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 28ms/step - accuracy: 0.5710 - loss: 0.6790 - val_accuracy: 0.5757 - val_loss: 0.6760\n",
            "Epoch 4/5\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 28ms/step - accuracy: 0.5694 - loss: 0.6780 - val_accuracy: 0.5808 - val_loss: 0.6743\n",
            "Epoch 5/5\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 26ms/step - accuracy: 0.5734 - loss: 0.6772 - val_accuracy: 0.5797 - val_loss: 0.6750\n",
            "Epoch 1/5\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 56ms/step - accuracy: 0.5867 - loss: 0.6717 - val_accuracy: 0.6011 - val_loss: 0.6637\n",
            "Epoch 2/5\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 38ms/step - accuracy: 0.6364 - loss: 0.6395 - val_accuracy: 0.5847 - val_loss: 0.6825\n",
            "Epoch 3/5\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 37ms/step - accuracy: 0.7045 - loss: 0.5733 - val_accuracy: 0.5787 - val_loss: 0.7198\n",
            "Epoch 4/5\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 36ms/step - accuracy: 0.7751 - loss: 0.4736 - val_accuracy: 0.5618 - val_loss: 0.8778\n",
            "Epoch 5/5\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 37ms/step - accuracy: 0.8355 - loss: 0.3653 - val_accuracy: 0.5533 - val_loss: 1.0725\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7dacf4717f50>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "mlm_model = keras.models.load_model(  # Carrega o modelo MLM salvo anteriormente do arquivo 'bert_mlm_stackoverflow.keras'.\n",
        "    \"bert_mlm_stackoverflow.keras\",\n",
        "    custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}  # Especifica objetos personalizados necessários para carregar o modelo corretamente.\n",
        ")\n",
        "pretrained_bert_model = keras.Model(  # Cria um novo modelo a partir do 'mlm_model'.\n",
        "    mlm_model.input,  # Usa a mesma entrada do modelo MLM.\n",
        "    mlm_model.get_layer(\"encoder_0_ffn_layernormalization\").output  # Define a saída como a da camada 'encoder_0_ffn_layernormalization', extraindo representações do encoder.\n",
        ")\n",
        "\n",
        "pretrained_bert_model.trainable = False  # Congela os pesos do modelo pré-treinado, impedindo que sejam atualizados durante o treinamento do classificador.\n",
        "\n",
        "\n",
        "def create_classifier_bert_model():\n",
        "    \"\"\"\n",
        "    Cria um modelo de classificação binária usando o BERT pré-treinado como extrator de características.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: Modelo compilado para classificação binária.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=\"int64\")  # Define a entrada como sequências de comprimento 'MAX_LEN' (256) com valores inteiros.\n",
        "    sequence_output = pretrained_bert_model(inputs)  # Passa a entrada pelo modelo BERT congelado para obter representações de sequência.\n",
        "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)  # Aplica pooling máximo global para reduzir a sequência a um único vetor.\n",
        "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)  # Adiciona uma camada densa com 64 unidades e ativação ReLU para aprendizado adicional.\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)  # Camada de saída com 1 unidade e ativação sigmoide para prever a probabilidade (0 ou 1).\n",
        "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")  # Cria o modelo com a entrada e saída definidas, nomeado como 'classification'.\n",
        "    optimizer = keras.optimizers.Adam()  # Usa o otimizador Adam com taxa de aprendizado padrão.\n",
        "    classifer_model.compile(  # Compila o modelo.\n",
        "        optimizer=optimizer,  # Define o otimizador.\n",
        "        loss=\"binary_crossentropy\",  # Usa entropia cruzada binária como função de perda, ideal para classificação binária.\n",
        "        metrics=[\"accuracy\"]  # Monitora a acurácia durante o treinamento.\n",
        "    )\n",
        "    return classifer_model  # Retorna o modelo compilado.\n",
        "\n",
        "classifer_model = create_classifier_bert_model()  # Instancia o modelo de classificação.\n",
        "classifer_model.summary()  # Exibe um resumo da arquitetura do modelo.\n",
        "\n",
        "classifer_model.fit(  # Treina o modelo de classificação com o BERT congelado.\n",
        "    train_classifier_ds,  # Usa o dataset de treino preparado anteriormente.\n",
        "    epochs=5,  # Treina por 5 épocas.\n",
        "    validation_data=test_classifier_ds,  # Usa o dataset de teste para validação durante o treinamento.\n",
        ")\n",
        "\n",
        "pretrained_bert_model.trainable = True  # Descongela os pesos do BERT, permitindo ajustes finos (fine-tuning).\n",
        "optimizer = keras.optimizers.Adam()  # Reinicia o otimizador Adam.\n",
        "classifer_model.compile(  # Recompila o modelo com os pesos descongelados.\n",
        "    optimizer=optimizer,\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "classifer_model.fit(  # Realiza o fine-tuning do modelo.\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA8Dgh-PIF1x"
      },
      "source": [
        "## Create an end-to-end model and evaluate it\n",
        "\n",
        "When you want to deploy a model, it's best if it already includes its preprocessing\n",
        "pipeline, so that you don't have to reimplement the preprocessing logic in your\n",
        "production environment. Let's create an end-to-end model that incorporates\n",
        "the `TextVectorization` layer inside evalaute method, and let's evaluate. We will pass raw strings as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gUeGbDLUIF1x",
        "outputId": "79810f25-aad7-46da-c22a-7c7ff151adca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - accuracy: 0.5522 - loss: 1.0759\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0725253820419312, 0.553309977054596]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "class ModelEndtoEnd(keras.Model):\n",
        "    \"\"\"\n",
        "    Modelo personalizado que estende 'keras.Model' para avaliar diretamente textos brutos.\n",
        "\n",
        "    Sobrescreve o método 'evaluate' para pré-processar os dados antes da avaliação.\n",
        "    \"\"\"\n",
        "    def evaluate(self, inputs):\n",
        "        \"\"\"\n",
        "        Avalia o modelo com um DataFrame bruto, codificando os textos antes de passar pelo modelo.\n",
        "\n",
        "        Args:\n",
        "            inputs (pd.DataFrame): DataFrame com colunas 'review' (textos) e 'sentiment' (rótulos).\n",
        "\n",
        "        Returns:\n",
        "            list: Resultados da avaliação (perda e métricas).\n",
        "        \"\"\"\n",
        "        features = encode(inputs.review.values)  # Codifica os textos brutos da coluna 'review' usando a função 'encode'.\n",
        "        labels = inputs.sentiment.values  # Extrai os rótulos da coluna 'sentiment'.\n",
        "        test_classifier_ds = (  # Cria um dataset TensorFlow a partir dos dados codificados.\n",
        "            tf.data.Dataset.from_tensor_slices((features, labels))  # Converte em pares (entrada, rótulo).\n",
        "            .shuffle(1000)  # Embaralha os dados.\n",
        "            .batch(config.BATCH_SIZE)  # Agrupa em lotes de tamanho 32.\n",
        "        )\n",
        "        return super().evaluate(test_classifier_ds)  # Chama o método 'evaluate' da classe pai com o dataset preparado.\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Define o modelo como construído, necessário para compatibilidade com o Keras.\n",
        "\n",
        "        Args:\n",
        "            input_shape: Formato da entrada (não usado diretamente aqui).\n",
        "        \"\"\"\n",
        "        self.built = True  # Marca o modelo como construído.\n",
        "\n",
        "def get_end_to_end(model):\n",
        "    \"\"\"\n",
        "    Cria um modelo end-to-end a partir de um modelo de classificação existente.\n",
        "\n",
        "    Args:\n",
        "        model (keras.Model): Modelo de classificação treinado.\n",
        "\n",
        "    Returns:\n",
        "        ModelEndtoEnd: Modelo end-to-end compilado.\n",
        "    \"\"\"\n",
        "    inputs = classifer_model.inputs[0]  # Obtém a primeira entrada do modelo de classificação.\n",
        "    outputs = classifer_model.outputs  # Obtém as saídas do modelo de classificação.\n",
        "    end_to_end_model = ModelEndtoEnd(inputs, outputs, name=\"end_to_end_model\")  # Cria uma instância do modelo end-to-end com a mesma entrada e saída.\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)  # Define o otimizador com a taxa de aprendizado de 'config'.\n",
        "    end_to_end_model.compile(  # Compila o modelo.\n",
        "        optimizer=optimizer,\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return end_to_end_model  # Retorna o modelo end-to-end pronto.\n",
        "\n",
        "end_to_end_classification_model = get_end_to_end(classifer_model)  # Cria o modelo end-to-end usando o 'classifer_model'.\n",
        "# Pass raw text dataframe to the model\n",
        "end_to_end_classification_model.evaluate(test_raw_classifier_ds)  # Avalia o modelo com o DataFrame bruto de teste, pré-processando os dados internamente."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Markov Chain for MLM\n",
        "\n",
        "### Description\n",
        "\n",
        "This cell defines the `build_markov_chain` function, which constructs a Markov Chain based on bigrams (default `n_gram=2`) for the Masked Language Modeling (MLM) task.\n",
        "\n",
        "### Input:\n",
        "- A list of texts (e.g., Stack Overflow questions).\n",
        "\n",
        "### Process:\n",
        "1. Applies `custom_standardization` (defined in the BERT code) to clean the text (e.g., remove HTML and punctuation).\n",
        "2. Splits the text into words and updates the vocabulary (`vocab`).\n",
        "3. For each sequence of words:\n",
        "   - Records the transition frequency from the current state (previous word) to the next state (following word).\n",
        "4. Normalizes the frequencies into transition probabilities.\n",
        "\n",
        "### Output:\n",
        "- A dictionary `markov_prob` containing transition probabilities.\n",
        "- A list `vocab` with the complete vocabulary.\n",
        "\n",
        "This function is the core of the Markov model, enabling masked word prediction based on local context."
      ],
      "metadata": {
        "id": "DXE1oUuTrs0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression  # Importa o modelo de regressão logística para classificação.\n",
        "from sklearn.metrics import accuracy_score  # Importa a função para calcular a acurácia do modelo.\n",
        "from collections import defaultdict  # Importa um dicionário que inicializa automaticamente valores padrão para chaves novas.\n",
        "\n",
        "# Construir Cadeia de Markov para MLM (bigramas)\n",
        "def build_markov_chain(texts, n_gram=2):\n",
        "    \"\"\"\n",
        "    Constrói uma Cadeia de Markov para modelar transições entre palavras com base em n-gramas.\n",
        "\n",
        "    Essa função cria um modelo probabilístico que prevê a próxima palavra com base nas anteriores (n-gramas).\n",
        "    Por padrão, usa bigramas (n_gram=2), onde o estado atual é uma palavra e o próximo estado é a palavra seguinte.\n",
        "\n",
        "    Args:\n",
        "        texts (list): Lista de textos (strings) usados para treinar o modelo.\n",
        "        n_gram (int): Tamanho do n-grama (quantas palavras anteriores considerar). Padrão é 2.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dicionário com probabilidades de transição entre palavras.\n",
        "        list: Lista de palavras únicas (vocabulário) encontradas nos textos.\n",
        "    \"\"\"\n",
        "    markov_chain = defaultdict(lambda: defaultdict(int))  # Cria um dicionário aninhado para contar transições.\n",
        "    vocab = set()  # Conjunto para armazenar o vocabulário único.\n",
        "    for text in texts:  # Itera sobre cada texto na lista:\n",
        "        cleaned_text = custom_standardization(tf.constant(text))  # Limpa o texto usando uma função de padronização externa.\n",
        "        # Convert the EagerTensor to a NumPy array and then decode it to a string\n",
        "        cleaned_text = cleaned_text.numpy().decode('utf-8')\n",
        "        words = cleaned_text.split()  # Divide o texto em palavras.\n",
        "        vocab.update(words)  # Adiciona as palavras ao vocabulário.\n",
        "        if len(words) < n_gram:  # Se o texto for muito curto para o n-grama, pula para o próximo.\n",
        "            continue\n",
        "        for i in range(len(words) - n_gram + 1):  # Para cada posição possível no texto:\n",
        "            current_state = \" \".join(words[i:i+n_gram-1])  # Forma o estado atual (ex.: uma palavra para n=2).\n",
        "            next_state = words[i+n_gram-1]  # Pega a próxima palavra como o próximo estado.\n",
        "            markov_chain[current_state][next_state] += 1  # Conta quantas vezes essa transição ocorre.\n",
        "    # Normalizar para probabilidades\n",
        "    markov_prob = {}  # Dicionário para armazenar as probabilidades normalizadas.\n",
        "    for current_state, transitions in markov_chain.items():  # Para cada estado e suas transições:\n",
        "        total = sum(transitions.values())  # Calcula o total de ocorrências de transições desse estado.\n",
        "        markov_prob[current_state] = {word: count/total for word, count in transitions.items()}  # Converte contagens em probabilidades.\n",
        "    return markov_prob, list(vocab)  # Retorna as probabilidades e o vocabulário."
      ],
      "metadata": {
        "id": "dEUgvrfFBYgi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked Word Prediction with Markov\n",
        "\n",
        "### Description\n",
        "\n",
        "This cell implements and tests masked word prediction using the Markov Chain.\n",
        "\n",
        "### `predict_masked_word` Function:\n",
        "\n",
        "#### **Input:**\n",
        "- A text containing `[mask]`\n",
        "- The transition probability dictionary `markov_prob`\n",
        "- The position of `[mask]` in the text\n",
        "- The n-gram size (default: `2`)\n",
        "\n",
        "#### **Process:**\n",
        "1. Cleans the text.\n",
        "2. Identifies the current state (words before `[mask]`).\n",
        "3. Returns the most probable word and its probability based on `markov_prob`.\n",
        "\n",
        "#### **Output:**\n",
        "- A tuple containing the predicted word and its probability (e.g., `(\"error\", 0.35)`).\n",
        "\n",
        "### **Testing:**\n",
        "- Builds the Markov Chain using all available data (`all_data[\"review\"]`).\n",
        "- Tests with the example `\"SyntaxError unexpected [mask]\"`, simulating BERT's callback.\n",
        "- Displays the prediction for qualitative analysis.\n",
        "\n",
        "This cell replicates the MLM task of BERT in a simplified manner, allowing direct comparison with BERT’s predictions."
      ],
      "metadata": {
        "id": "aJAZsihpseoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para prever palavra mascarada com Markov\n",
        "def predict_masked_word(text, markov_prob, mask_position, n_gram=2):\n",
        "    \"\"\"\n",
        "    Prevê a palavra mais provável em uma posição mascarada usando a Cadeia de Markov.\n",
        "\n",
        "    Dado um texto com '[mask]' em alguma posição, essa função usa as palavras anteriores à máscara\n",
        "    para prever qual palavra tem maior probabilidade de aparecer ali, com base no modelo Markov.\n",
        "\n",
        "    Args:\n",
        "        text (str): Texto com '[mask]' indicando a posição a ser prevista.\n",
        "        markov_prob (dict): Dicionário de probabilidades de transição do modelo Markov.\n",
        "        mask_position (int): Índice da posição da máscara no texto.\n",
        "        n_gram (int): Tamanho do n-grama usado no modelo. Padrão é 2.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A palavra prevista e sua probabilidade (ou 'unknown' e 0.0 se não houver previsão).\n",
        "    \"\"\"\n",
        "    cleaned_text = custom_standardization(tf.constant(text)).numpy().decode('utf-8')  # Limpa o texto com a função de padronização.\n",
        "    words = cleaned_text.split()  # Divide o texto em palavras.\n",
        "    if len(words) < n_gram or mask_position < n_gram-1 or mask_position >= len(words):  # Verifica se há contexto suficiente.\n",
        "        return \"unknown\", 0.0  # Retorna 'unknown' se o texto for curto ou a máscara estiver mal posicionada.\n",
        "    current_state = \" \".join(words[mask_position-n_gram+1:mask_position])  # Forma o estado atual com palavras antes da máscara.\n",
        "    if current_state in markov_prob:  # Se o estado atual estiver no modelo:\n",
        "        next_words = markov_prob[current_state]  # Pega as possíveis próximas palavras e suas probabilidades.\n",
        "        if next_words:  # Se houver transições registradas:\n",
        "            predicted_word = max(next_words.items(), key=lambda x: x[1])[0]  # Escolhe a palavra mais provável.\n",
        "            probability = next_words[predicted_word]  # Pega a probabilidade dessa palavra.\n",
        "            return predicted_word, probability  # Retorna a previsão.\n",
        "    return \"unknown\", 0.0  # Retorna 'unknown' se não houver previsão possível.\n",
        "\n",
        "# Testar MLM com Markov\n",
        "markov_prob, vocab = build_markov_chain(all_data[\"review\"].values)  # Constrói o modelo Markov com os dados de reviews.\n",
        "sample_text = \"SyntaxError unexpected [mask]\"  # Texto de exemplo com uma máscara.\n",
        "cleaned_text = custom_standardization(tf.constant(sample_text)).numpy().decode('utf-8')\n",
        "words = cleaned_text.split()  # Limpa e divide o texto em palavras.\n",
        "mask_position = words.index(\"[mask]\")  # Encontra o índice da máscara.\n",
        "predicted_word, prob = predict_masked_word(sample_text, markov_prob, mask_position)  # Prevê a palavra mascarada.\n",
        "print(f\"Markov Prediction for '{sample_text}':\")  # Exibe o texto original.\n",
        "print(f\"Predicted word: '{predicted_word}', Probability: {prob:.4f}\")  # Exibe a palavra prevista e sua probabilidade."
      ],
      "metadata": {
        "id": "0YuWQKxCroAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a84450-773a-410d-e63b-c9b96f6c8a66"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Markov Prediction for 'SyntaxError unexpected [mask]':\n",
            "Predicted word: 'token', Probability: 0.1760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction for Classification\n",
        "\n",
        "### Description\n",
        "\n",
        "This cell prepares data for a binary classification task using features derived from the Markov Chain.\n",
        "\n",
        "### `extract_markov_features` Function:\n",
        "\n",
        "#### **Input:**\n",
        "- A list of texts\n",
        "- The transition probability dictionary `markov_prob`\n",
        "\n",
        "#### **Process:**\n",
        "1. Computes the average transition probabilities of bigrams in each text.\n",
        "2. Generates a single numerical feature per text.\n",
        "\n",
        "#### **Output:**\n",
        "- A NumPy array with shape `(n_samples, 1)`, ready for use in a classifier.\n",
        "\n",
        "### **Preparation:**\n",
        "- Generates features for the training set (`X_train_markov`) and test set (`X_test_markov`).\n",
        "- Defines labels `y_train` and `y_test` based on sentiment (`Score >= 1` or `< 1`).\n",
        "- Keeps `test_raw_classifier_ds` as the test DataFrame for compatibility with BERT.\n",
        "\n",
        "This approach transforms Markov probabilities into a simple representation for classification, contrasting with BERT’s rich embeddings."
      ],
      "metadata": {
        "id": "bj9YzRHJsk1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrair features para classificação (média das probabilidades de transição)\n",
        "def extract_markov_features(texts, markov_prob, n_gram=2):\n",
        "    \"\"\"\n",
        "    Extrai uma característica numérica para cada texto baseada nas probabilidades da Cadeia de Markov.\n",
        "\n",
        "    Calcula a média das probabilidades de transição entre palavras em cada texto,\n",
        "    usando essas médias como features para o classificador.\n",
        "\n",
        "    Args:\n",
        "        texts (list): Lista de textos para extrair características.\n",
        "        markov_prob (dict): Dicionário de probabilidades do modelo Markov.\n",
        "        n_gram (int): Tamanho do n-grama. Padrão é 2.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array com uma característica por texto (média das probabilidades).\n",
        "    \"\"\"\n",
        "    features = []  # Lista para armazenar as características.\n",
        "    for text in texts:  # Para cada texto:\n",
        "        cleaned_text = custom_standardization(tf.constant(text)).numpy().decode('utf-8')  # Limpa o texto.\n",
        "        words = cleaned_text.split()  # Divide em palavras.\n",
        "        if len(words) < n_gram:  # Se o texto for muito curto, atribui 0.0 como feature.\n",
        "            features.append(0.0)\n",
        "            continue\n",
        "        prob_sum = 0  # Soma das probabilidades de transição.\n",
        "        count = 0  # Contador de transições válidas.\n",
        "        for i in range(len(words) - n_gram + 1):  # Para cada n-grama no texto:\n",
        "            current_state = \" \".join(words[i:i+n_gram-1])  # Estado atual.\n",
        "            next_word = words[i+n_gram-1]  # Próxima palavra.\n",
        "            if current_state in markov_prob and next_word in markov_prob[current_state]:  # Se a transição existe no modelo:\n",
        "                prob_sum += markov_prob[current_state][next_word]  # Adiciona a probabilidade.\n",
        "                count += 1  # Incrementa o contador.\n",
        "        features.append(prob_sum / count if count > 0 else 0.0)  # Calcula a média ou usa 0.0 se não houver transições.\n",
        "    return np.array(features).reshape(-1, 1)  # Retorna as features como um array NumPy com uma coluna.\n",
        "\n",
        "# Preparar dados para classificação\n",
        "X_train_markov = extract_markov_features(train_df[\"review\"].values, markov_prob)  # Extrai features para o treino.\n",
        "X_test_markov = extract_markov_features(test_df[\"review\"].values, markov_prob)  # Extrai features para o teste.\n",
        "y_train = train_df[\"sentiment\"].values  # Rótulos de sentimento para o treino.\n",
        "y_test = test_df[\"sentiment\"].values  # Rótulos de sentimento para o teste.\n",
        "\n",
        "test_raw_classifier_ds = test_df  # Armazena o DataFrame de teste para uso posterior."
      ],
      "metadata": {
        "id": "UFSc6CNYroez"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation of the Markov Classifier\n",
        "\n",
        "### Description\n",
        "\n",
        "This cell trains and evaluates a classifier based on features extracted from the Markov model.\n",
        "\n",
        "### **Training:**\n",
        "- Uses `LogisticRegression` with the extracted features (`X_train_markov`) and labels (`y_train`).\n",
        "\n",
        "### **Evaluation:**\n",
        "- Computes predictions for the test set (`X_test_markov`).\n",
        "- Measures accuracy using `accuracy_score`.\n",
        "\n",
        "### **Output:**\n",
        "- Prints the accuracy of the Markov model, enabling a quantitative comparison with BERT.\n",
        "\n",
        "Logistic regression is a simple yet effective choice for classification using a single feature, reflecting the minimalistic approach of the Markov model."
      ],
      "metadata": {
        "id": "iM8dU5LLtHs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar classificador com Markov\n",
        "markov_classifier = LogisticRegression(max_iter=1000)  # Cria um modelo de regressão logística com até 1000 iterações.\n",
        "markov_classifier.fit(X_train_markov, y_train)  # Treina o modelo com os dados de treino.\n",
        "\n",
        "# Avaliar Markov\n",
        "y_pred_markov = markov_classifier.predict(X_test_markov)  # Faz previsões no conjunto de teste.\n",
        "markov_accuracy = accuracy_score(y_test, y_pred_markov)  # Calcula a acurácia comparando previsões e rótulos reais.\n",
        "print(f\"\\nAcurácia do modelo Markov (classificação): {markov_accuracy:.4f}\")  # Exibe a acurácia.\n",
        "\n",
        "# Comparar com BERT (assumindo que você já rodou o código BERT)\n",
        "bert_accuracy = end_to_end_classification_model.evaluate(test_raw_classifier_ds)[1]  # Avalia o modelo BERT e pega a acurácia.\n",
        "print(f\"Acurácia do modelo BERT (classificação): {bert_accuracy:.4f}\")  # Exibe a acurácia do BERT.\n",
        "\n",
        "# Comparação\n",
        "print(\"\\nComparação dos Resultados (Classificação Binária):\")  # Inicia a seção de comparação.\n",
        "print(f\"Modelo Markov - Acurácia: {markov_accuracy:.4f}\")  # Exibe a acurácia do Markov.\n",
        "print(f\"Modelo BERT - Acurácia: {bert_accuracy:.4f}\")  # Exibe a acurácia do BERT.\n",
        "print(f\"Diferença (BERT - Markov): {bert_accuracy - markov_accuracy:.4f}\")  # Mostra a diferença entre as acurácias.\n",
        "\n",
        "# Comparação no MLM (qualitativa)\n",
        "print(\"\\nComparação no MLM (exemplo qualitativo):\")  # Inicia a comparação qualitativa do MLM.\n",
        "print(f\"BERT Prediction (Epoch 5): 'error' com probabilidade 0.4701\")  # Exemplo fixo da previsão do BERT.\n",
        "print(f\"Markov Prediction: '{predicted_word}' com probabilidade {prob:.4f}\")  # Exibe a previsão do Markov."
      ],
      "metadata": {
        "id": "_A32jZLHro4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ff9a16-09ed-4e4b-9bf1-9c9ecf802947"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acurácia do modelo Markov (classificação): 0.5303\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 12ms/step - accuracy: 0.5524 - loss: 1.0756\n",
            "Acurácia do modelo BERT (classificação): 0.5533\n",
            "\n",
            "Comparação dos Resultados (Classificação Binária):\n",
            "Modelo Markov - Acurácia: 0.5303\n",
            "Modelo BERT - Acurácia: 0.5533\n",
            "Diferença (BERT - Markov): 0.0230\n",
            "\n",
            "Comparação no MLM (exemplo qualitativo):\n",
            "BERT Prediction (Epoch 5): 'error' com probabilidade 0.4701\n",
            "Markov Prediction: 'token' com probabilidade 0.1760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy Comparison: Markov vs BERT\n",
        "\n",
        "### Description\n",
        "\n",
        "This cell generates a bar chart to visually compare the accuracies of the Markov and BERT models.\n",
        "\n",
        "### **Chart:**\n",
        "- A bar chart is created using `matplotlib`, displaying the accuracies of the two models.\n",
        "- The bars represent the \"Markov\" and \"BERT\" models with distinct colors (blue and green).\n",
        "- The y-axis is limited between 0 and 1 to reflect the accuracy range.\n",
        "\n",
        "### **Purpose:**\n",
        "- Provide a clear and straightforward visualization of the relative performance of the two models.\n",
        "- Enable a quick comparison between the Markov chain-based approach and the BERT model.\n",
        "\n",
        "This chart is a useful tool to highlight the performance difference between a simple probabilistic model (Markov) and an advanced deep learning model (BERT)."
      ],
      "metadata": {
        "id": "ArhWkI0FkI_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt  # Importa a biblioteca para criar gráficos.\n",
        "\n",
        "# Dados para o gráfico\n",
        "models = ['Markov', 'BERT']  # Nomes dos modelos a serem comparados.\n",
        "accuracies = [markov_accuracy, bert_accuracy]  # Acurácias dos dois modelos.\n",
        "\n",
        "# Criação do gráfico de barras\n",
        "plt.figure(figsize=(8, 6))  # Define o tamanho da figura (largura, altura).\n",
        "plt.bar(models, accuracies, color=['blue', 'green'])  # Cria barras com cores distintas para cada modelo.\n",
        "plt.ylim(0, 1)  # Define o limite do eixo y entre 0 e 1 (acurácia varia de 0 a 1).\n",
        "plt.title('Comparação de Acurácia: Markov vs BERT')  # Título do gráfico.\n",
        "plt.xlabel('Modelos')  # Rótulo do eixo x.\n",
        "plt.ylabel('Acurácia')  # Rótulo do eixo y.\n",
        "plt.show()  # Exibe o gráfico."
      ],
      "metadata": {
        "id": "-RpwGWbzkJRl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "46e95c58-3ac5-4b46-dfb4-ebc43c45ced5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAIkCAYAAAANhKPgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAARW9JREFUeJzt3Xl8jOf+//H3JGSxJNSSSIQUpZRaolJblZM2KrW3Yimhpa29UpQuUtUjunNoq3pqqVJKVRfbUapK9VAa1FY735KIIrEmkbl+f/hljjFBoknGzev5eNyPR+ea677uzz0Z03euue8rNmOMEQAAAGBBHu4uAAAAALhRhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgHgFvT999/rn//8p86dO+fuUgAgXxFmARSIzp07q3jx4ho6dKhOnjypEiVK6NSpU/l+3OnTp8tms+nAgQP5fqybxZ49e9SxY0cFBASoSJEiud7/dnjNss7x119/dXcpAP4mwixwFXv37tUzzzyjSpUqycfHR35+fmrcuLEmTJig8+fPu7s8S9m+fbtWrVql0aNH65tvvlGpUqUUERGhEiVKuLu0v2Xx4sWy2WwKCgqS3W53dzmSpLS0NHXq1EkDBw5U79693V3ONfXs2VM2m01+fn7Z/pvavXu3bDabbDab3n77bTdUeGvIeg2ztqJFi6pGjRp6/fXXXWbus34m2W0+Pj6OfqtWrXJ6ztPTU2XLltVjjz2mHTt2SJJeffXVq451+fbggw8W5MuBW1AhdxcA3IwWLVqkxx9/XN7e3urRo4dq1qyp9PR0rVmzRsOGDdO2bds0ZcoUd5dpGZUqVdLGjRsVHBys5557TomJiSpXrpy7y/rbZs2apdDQUB04cEArV65URESEu0vStm3b1KtXLw0cOPCGx+jevbs6d+4sb2/vPKwse4UKFdK5c+f07bffqlOnTk7PzZo1Sz4+Prpw4UK+13Gre+ihh9SjRw9J0pkzZ/TTTz/plVde0ebNmzVv3jynvt7e3vr3v//tMoanp6dL26BBg3TfffcpIyNDW7Zs0eTJk7Vq1Sr9/vvv6tChg6pUqeLoe+bMGfXt21ft27dXhw4dHO0BAQF5dZq4TRFmgSvs379fnTt3VsWKFbVy5Uqn0NW/f3/t2bNHixYtcmOF+cdutys9Pd1pBiYv+Pj4KDg4WJLk4eGhoKCgPB3fHc6ePauvv/5a8fHxmjZtmmbNmuWWMHv27FkVLVrU8bhevXqqV6/e3xrT09Mz2+CSH7y9vdW4cWN9/vnnLmF29uzZioqK0pdffplnx7tw4YK8vLzybDyrqFq1qp544gnH42effVbp6elasGCBLly44PRvvlChQk59r6Vp06Z67LHHHI+rVaumvn376tNPP9Xw4cN17733Op47fvy4+vbtq3vvvTfH4wM5wWUGwBXefPNNnTlzRp988km2s4dVqlTR4MGDHY8vXryoMWPGqHLlyvL29lZoaKhefPFFpaWlOe0XGhqqRx99VKtWrVL9+vXl6+urWrVqadWqVZKkBQsWqFatWvLx8VFYWJh+++03p/179uypYsWKad++fYqMjFTRokUVFBSk1157TcYYp75vv/22GjVqpFKlSsnX11dhYWGaP3++y7nYbDYNGDBAs2bN0j333CNvb28tXbo0V2NI0meffaYGDRqoSJEiKlmypB544AH95z//cTz/1VdfqVWrVgoKCpK3t7cqV66sMWPGKDMz02WsefPmKSwsTL6+vipdurSeeOIJ/fnnn9ke90rbtm1TixYt5Ovrq/Lly+v111+/6tf/S5YsUdOmTVW0aFEVL15cUVFR2rZtW46Ok3VO58+f1+OPP67OnTs7QsGVLly4oFdffVVVq1aVj4+PypUrpw4dOmjv3r2S/vd1bdb7IMuBAwdks9k0ffp0R1vWe2Dv3r1q1aqVihcvrm7dukmSfvrpJz3++OOqUKGCvL29FRISoiFDhmT79f3OnTvVqVMnlSlTRr6+vqpWrZpeeuklx/PZXTP79ddfKyoq6ro/w3Pnzmnnzp06fvx4jl/Lrl27asmSJU7XUG/YsEG7d+9W165dXfqfOHFCQ4cOVa1atVSsWDH5+fnpkUce0ebNm536Zb22c+bM0csvv6zg4GAVKVJEqamp2dZx8uRJNWjQQOXLl9euXbskSceOHdNTTz2lgIAA+fj4qHbt2poxY4Zjn4yMDN1xxx3q1auXy3ipqany8fHR0KFDr3ruNWvWVPPmzV3a7Xa7goODnYLinDlzFBYWpuLFi8vPz0+1atXShAkTrjr29QQGBspms6lQobyb12ratKkkOd7fQEEgzAJX+Pbbb1WpUiU1atQoR/179+6tUaNGqV69enrvvffUrFkzxcfHq3Pnzi599+zZo65du6p169aKj4/XyZMn1bp1a82aNUtDhgzRE088odGjR2vv3r3q1KmTSxDLzMxUy5YtFRAQoDfffFNhYWGKi4tTXFycU78JEyaobt26eu211zR27FgVKlRIjz/+eLYzyitXrtSQIUMUHR2tCRMmKDQ0NFdjjB49Wt27d1fhwoX12muvafTo0QoJCdHKlSsdfaZOnarixYsrNjZW48ePV1hYmEaNGqURI0Y4jTV9+nR16tRJnp6eio+PV58+fbRgwQI1adLkujeLJSYmqnnz5kpISNCIESP03HPP6dNPP832f/YzZ85UVFSUihUrpjfeeEOvvPKKtm/friZNmuT4pqdZs2apefPmCgwMVOfOnXX69Gl9++23Tn0yMzP16KOPavTo0QoLC9M777yjwYMHKyUlRb///nuOjnOlixcvKjIyUmXLltXbb7+tjh07Srr0S8DZs2fVt29fTZw4UQ8//LAmTpzo+Go5y5YtWxQeHq6VK1eqT58+mjBhgtq1a+dS+5WmT5+uYsWKKTY2VhMmTLjqz3D9+vWqXr26Jk2alONz6tChg2w2mxYsWOBomz17tu6+++5sZ5n37dunhQsX6tFHH9W7776rYcOGaevWrWrWrJmOHDni0n/MmDFatGiRhg4dqrFjx2Y7M3v8+HG1aNFCSUlJ+vHHH1WtWjWdP39eDz74oGbOnKlu3brprbfekr+/v3r27Ol4XxUuXFjt27fXwoULlZ6e7jTmwoULlZaWlu1nQZbo6GitXr1aiYmJTu1r1qzRkSNHHPsuX75cXbp0UcmSJfXGG29o3LhxevDBB7V27dprvLL/c+HCBR0/flzHjx/XwYMHNXv2bM2YMUNdu3bNNsxm9b18u9ovAZfL+vdTsmTJHNUF5AkDwCElJcVIMm3bts1R/4SEBCPJ9O7d26l96NChRpJZuXKlo61ixYpGkvn5558dbcuWLTOSjK+vrzl48KCj/aOPPjKSzA8//OBoi4mJMZLMwIEDHW12u91ERUUZLy8vk5yc7Gg/d+6cUz3p6emmZs2apkWLFk7tkoyHh4fZtm2by7nlZIzdu3cbDw8P0759e5OZmenU3263O/777NmzLuM/88wzpkiRIubChQuO8cuWLWtq1qxpzp8/7+j33XffGUlm1KhRLmNc7rnnnjOSzH//+19H27Fjx4y/v7+RZPbv32+MMeb06dOmRIkSpk+fPk77JyYmGn9/f5f27CQlJZlChQqZjz/+2NHWqFEjl/fN1KlTjSTz7rvvuoyR9fr88MMPLj9rY4zZv3+/kWSmTZvmaMt6D4wYMcJlvDNnzri0vf7668Zmszm9tx544AFTvHhxp7bL6zHGmGnTpjm9Zsa4vh+Mcf0ZXn4+cXFxLv2vFBMTY4oWLWqMMeaxxx4z//jHP4wxxmRmZprAwEAzevRox+vw1ltvOfa7cOGCy/tt//79xtvb27z22msutVSqVMml/qxz3LBhgzl69Ki55557TKVKlcyBAwccfcaPH28kmc8++8zRlp6ebho2bGiKFStmUlNTjTH/+3f87bffOh2jVatWplKlStd8DXbt2mUkmYkTJzq19+vXzxQrVsxR9+DBg42fn5+5ePHiNcfLjqRst3bt2jn97Iz533ssuy0yMtLRL+u1nTp1qklOTjZHjhwxS5cuNVWqVDE2m82sX7/epY7k5OQcvzeA3GBmFrhM1sxD8eLFc9R/8eLFkqTY2Fin9ueff16SXGYxa9SooYYNGzoeh4eHS5JatGihChUquLTv27fP5ZgDBgxw/HfWZQLp6en6/vvvHe2+vr6O/z558qRSUlLUtGlTbdq0yWW8Zs2aqUaNGi7tORlj4cKFstvtGjVqlDw8nD9ObDab478vXx7q9OnTOn78uJo2ber4SlqSfv31Vx07dkz9+vVzun4vKipKd99993WvU168eLHuv/9+NWjQwNFWpkwZx9fwWZYvX65Tp06pS5cuTrNOnp6eCg8P1w8//HDN40iXvu718PBwzIpKUpcuXbRkyRKdPHnS0fbll1+qdOnS2d6Mdfnrk1t9+/Z1abv8ulm73a4LFy4oMjJSxhjHJSvJyclavXq1nnzySaf3W07qufz9cLWfoSQ9+OCDMsbo1VdfzdU5de3aVatWrVJiYqJWrlypxMTEbC8xkC5dZ5v1fsvMzNRff/2lYsWKqVq1atm+x2NiYpzqv9z//d//qVmzZsrIyNDq1atVsWJFx3OLFy9WYGCgunTp4mgrXLiwBg0apDNnzujHH3+UdOnfb+nSpTV37lxHv5MnT2r58uWKjo6+5nlXrVpVderUcdo3MzNT8+fPV+vWrR11lyhRQmfPntXy5cuvOd7VtG3bVsuXL9fy5cv19ddfa+TIkVq6dKm6du3qcpmSj4+Po+/l27hx41zGffLJJ1WmTBkFBQWpZcuWSklJ0cyZM3XffffdUJ3AjeAGMOAyfn5+ki79zzonDh48KA8PD6c7dqVL16KVKFFCBw8edGq/MkD4+/tLkkJCQrJtvzwYSZdunqpUqZJTW9WqVSXJ6evx7777Tq+//roSEhKcrt3NLrDceeed2Z5bTsbYu3evPDw8sg3Dl9u2bZtefvllrVy50uWrypSUFElyvFbVqlVz2f/uu+/WmjVrrnmMgwcPOn4JuNyV4+3evVvSpQCSnaz3wLVkXSP8119/6a+//pIk1a1bV+np6Zo3b56efvppSZden2rVquXpNYmFChVS+fLlXdqPHDmi119/Xd9++62OHj3qdC1r1muc9ctRzZo1c33cnPwM/46sa4Dnzp2rhIQE3XfffapSpUq2l33Y7XZNmDBBH3zwgfbv3+90rqVKlXLpf7X3uHRp5YZChQppx44dCgwMdHru4MGDuuuuu1x+UatevbrjeenSz6Rjx46aPXu20tLS5O3trQULFigjI+O6YVa6dKnBiy++qD///FPBwcFatWqVjh075rRvv3799MUXX+iRRx5RcHCwHn74YXXq1EktW7a87viSVL58eacbFNu0aaNSpUpp6NCh+u6779S6dWvHc56enjm+mXHUqFFq2rSpzpw5o6+++srxix5QkHjHAZfx8/NTUFBQrq9nzOks29XuEL9a+5UzJjnx008/qU2bNvLx8dEHH3ygxYsXa/ny5dnOwEjKdsYqt2Ncy6lTp9SsWTNt3rxZr732mr799lstX75cb7zxhiQV+PqsWcebOXNmtrNPX3/99TX33717tzZs2KA1a9borrvucmxNmjSRdOla2ty42nsnu5vjJOdZycvP6aGHHtKCBQs0fPhwLVu2TOvWrdMXX3zhdM43qiB+ht7e3urQoYNmzJihr7766qqzspI0duxYxcbG6oEHHtBnn32mZcuWafny5brnnnuyreVqs7LSpet1T5069bdupJLkuG56yZIlkqQvvvhCd999t2rXrn3dfaOjo2WMcSyR9cUXX8jf398pqJYtW1YJCQn65ptv1KZNG/3www965JFHFBMTc8M1/+Mf/5AkrV69+obHqFWrliIiItSuXTvNmDFDbdq0UZ8+fXT48OEbHhPILWZmgSs8+uijmjJlitatW+d0SUB2KlasKLvdrt27dztmayQpKSlJp06dcvrKMi/Y7Xbt27fPMRsrSX/88YckOW7c+vLLL+Xj46Nly5Y5rRM6bdq0HB8np2NUrlxZdrtd27dvV506dbIda9WqVfrrr7+0YMECPfDAA472/fv3O/XLeq127drlMmu6a9eu676WFStWdMy6XrnvlTVLl8LBjSylNWvWLBUuXFgzZ850+SVkzZo1+te//qVDhw6pQoUKqly5sv773/8qIyNDhQsXzna8rBtlrrzB7cpZ/WvZunWrtm/frs8++8zpsoorZ1CzZvVz+8taTn+Gf1fXrl01depUeXh4XPOmqfnz56t58+b65JNPnNpPnTql0qVL5+qYAwcOVJUqVTRq1Cj5+/s73dBWsWJFbdmyRXa73ekXiKzLKi5/Tz7wwAMqV66c5s6dqyZNmmjlypVOK0Rcy5133qkGDRpo7ty5GjBggBYsWKB27dq5rPPr5eWl1q1bq3Xr1rLb7erXr58++ugjvfLKKy7fDuXExYsXJV1a/zWvjBs3Tl999ZX++c9/avLkyXk2LnAtzMwCVxg+fLiKFi2q3r17KykpyeX5vXv3OmZxWrVqJUkaP368U593331X0qXrPfPa5XeJG2M0adIkFS5c2DHL4unpKZvN5jSzd+DAAS1cuDDHx8jpGO3atZOHh4dee+01lxmxrBncrMB3+Yxuenq6PvjgA6f+9evXV9myZTV58mSnyxqWLFmiHTt2XPe1bNWqlX755RetX7/e0ZacnOwyUxoZGSk/Pz+NHTtWGRkZLuMkJydf8zizZs1S06ZNFR0drccee8xpGzZsmCTp888/lyR17NhRx48fz/bO/qzXo2LFivL09HSZHbvy9bmWrNndy8/Hbrfrvffec+pXpkwZPfDAA5o6daoOHTqUbT3ZyenPULqxpbmyNG/eXGPGjNGkSZNcvvK/sp4r6503b16Ol3C70iuvvKKhQ4dq5MiR+vDDDx3trVq1UmJiotP1rBcvXtTEiRNVrFgxNWvWzNHu4eGhxx57TN9++61mzpypixcv5ugSgyzR0dH65ZdfNHXqVB0/ftxl36zLWS4/XtYarlcuA5hTWStY5GT2OKcqV66sjh07avr06S4rNAD5hZlZ4AqVK1fW7NmzFR0drerVqzv9BbCff/5Z8+bNU8+ePSVd+p9ATEyMpkyZ4vgqdv369ZoxY4batWuX7fqRf4ePj4+WLl2qmJgYhYeHa8mSJVq0aJFefPFFlSlTRtKlAP3uu++qZcuW6tq1q44dO6b3339fVapU0ZYtW3J0nJyOUaVKFb300ksaM2aMmjZtqg4dOsjb21sbNmxQUFCQ4uPj1ahRI5UsWVIxMTEaNGiQbDabZs6c6RJGChcurDfeeEO9evVSs2bN1KVLFyUlJTmWCxsyZMg1ax4+fLhmzpypli1bavDgwSpatKimTJnimF3L4ufnpw8//FDdu3dXvXr11LlzZ5UpU0aHDh3SokWL1Lhx46suK/Xf//5Xe/bscboJ73LBwcGqV6+eZs2apRdeeEE9evTQp59+qtjYWK1fv15NmzbV2bNn9f3336tfv35q27at/P399fjjj2vixImy2WyqXLmyvvvuOx07dixHPyvp0jWclSpV0tChQ3XkyBEVL15cX375ZbZLKf3rX/9SkyZNVK9ePT399NO68847deDAAS1atEgJCQnZjp/Tn6F0aWmu5s2bKy4uLtc3gXl4eOjll1++br9HH31Ur732mnr16qVGjRpp69atmjVrlsv15Lnx1ltvKSUlRf3791fx4sX1xBNP6Omnn9ZHH32knj17auPGjQoNDdX8+fO1du1ajR8/3uVG0ejoaE2cOFFxcXGqVauW07c119OpUycNHTpUQ4cO1R133OHyrUHv3r114sQJtWjRQuXLl9fBgwc1ceJE1alTJ0fH+eOPP/TZZ59JuvQLxy+//KIZM2aoSpUq6t69u1PfixcvOvpeqX379k43G2Zn2LBh+uKLLzR+/PhsbxoD8lzBL6AAWMMff/xh+vTpY0JDQ42Xl5cpXry4ady4sZk4caLTcjYZGRlm9OjR5s477zSFCxc2ISEhZuTIkS5L3lSsWNFERUW5HEeS6d+/v1NbdssRZS1jtHfvXvPwww+bIkWKmICAABMXF+eyTNEnn3xi7rrrLuPt7W3uvvtuM23aNBMXF2eu/Cef3bFzO4Yxl5agqlu3rmMJn2bNmpnly5c7nl+7dq25//77ja+vrwkKCjLDhw93LGd05ZJUc+fONXXr1jXe3t7mjjvuMN26dTP/93//l22NV9qyZYtp1qyZ8fHxMcHBwWbMmDHmk08+cVlmyphLSwtFRkYaf39/4+PjYypXrmx69uxpfv3116uOP3DgQCPJ7N2796p9Xn31VSPJbN682RhzaUmrl156yfH+CAwMNI899pjTGMnJyaZjx46mSJEipmTJkuaZZ54xv//+e7ZLc2UtZXWl33//3bRo0cIUK1bMlClTxjz77LNm69atLmNk9W3fvr0pUaKE8fHxMdWqVTOvvPKK4/nslubK6c/wRpfmupqrLc31/PPPm3LlyhlfX1/TuHFjs27dOtOsWTPTrFkzl1rmzZvnMu7lS3NlyczMNF26dDGFChUyCxcuNMZcWoatV69epnTp0sbLy8vUqlXL5fXMYrfbTUhIiJFkXn/99eue/5UaN26c7VJ/xhgzf/588/DDD5uyZcsaLy8vU6FCBfPMM8+Yo0ePXnfcrH+XWZunp6cpX768efrpp01SUpJT32stzXX5e+Jar60xxjz44IPGz8/PnDp1ytHG0lzILzZjbuAOEwAFrmfPnpo/f36eXt+W1w4cOKCHHnpI27Ztuy3/ZCgAoOBxzSyAPBMaGqpixYpddxktAADyCtfMAsgTr776qkqXLq3du3ff1LPHAIBbC2EWQJ749NNPdeTIETVv3lyRkZHuLgcAcJtw62UGq1evVuvWrRUUFCSbzZajpYNWrVqlevXqydvbW1WqVNH06dPzvU7gZjB9+vSbesZz3759unDhgpYsWeKyPiYAAPnFrWH27Nmzql27tt5///0c9d+/f7+ioqLUvHlzJSQk6LnnnlPv3r21bNmyfK4UAAAAN6ObZjUDm82mr776Su3atbtqnxdeeEGLFi1y+us1nTt31qlTp7R06dICqBIAAAA3E0utZrBu3TqXhaQjIyO1bt06N1UEAAAAd7LUDWCJiYkKCAhwagsICFBqaqrOnz8vX19fl33S0tKc/tSf3W7XiRMnVKpUKcefgAQAAMDNwxij06dPKygoSB4e1557tVSYvRHx8fEaPXq0u8sAAABALh0+fFjly5e/Zh9LhdnAwEAlJSU5tSUlJcnPzy/bWVlJGjlypGJjYx2PU1JSVKFCBR0+fFh+fn75Wi8AAAByLzU1VSEhISpevPh1+1oqzDZs2FCLFy92alu+fLkaNmx41X28vb2zXSbIz8+PMAsAAHATy8kloW69AezMmTNKSEhQQkKCpEtLbyUkJOjQoUOSLs2q9ujRw9H/2Wef1b59+zR8+HDt3LlTH3zwgb744gsNGTLEHeUDAADAzdwaZn/99VfVrVtXdevWlSTFxsaqbt26GjVqlCTp6NGjjmArSXfeeacWLVqk5cuXq3bt2nrnnXf073//m782BAAAcJu6adaZLSipqany9/dXSkoKlxkAAADchHKT1yy1ziwAAABwOcIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALMvtYfb9999XaGiofHx8FB4ervXr11+z//jx41WtWjX5+voqJCREQ4YM0YULFwqoWgAAANxM3Bpm586dq9jYWMXFxWnTpk2qXbu2IiMjdezYsWz7z549WyNGjFBcXJx27NihTz75RHPnztWLL75YwJUDAADgZuDWMPvuu++qT58+6tWrl2rUqKHJkyerSJEimjp1arb9f/75ZzVu3Fhdu3ZVaGioHn74YXXp0uW6s7kAAAC4NbktzKanp2vjxo2KiIj4XzEeHoqIiNC6deuy3adRo0bauHGjI7zu27dPixcvVqtWra56nLS0NKWmpjptAAAAuDUUcteBjx8/rszMTAUEBDi1BwQEaOfOndnu07VrVx0/flxNmjSRMUYXL17Us88+e83LDOLj4zV69Og8rR0AAAA3B7ffAJYbq1at0tixY/XBBx9o06ZNWrBggRYtWqQxY8ZcdZ+RI0cqJSXFsR0+fLgAKwYAAEB+ctvMbOnSpeXp6amkpCSn9qSkJAUGBma7zyuvvKLu3burd+/ekqRatWrp7Nmzevrpp/XSSy/Jw8M1m3t7e8vb2zvvTwAAAABu57aZWS8vL4WFhWnFihWONrvdrhUrVqhhw4bZ7nPu3DmXwOrp6SlJMsbkX7EAAAC4KbltZlaSYmNjFRMTo/r166tBgwYaP368zp49q169ekmSevTooeDgYMXHx0uSWrdurXfffVd169ZVeHi49uzZo1deeUWtW7d2hFoAAADcPtwaZqOjo5WcnKxRo0YpMTFRderU0dKlSx03hR06dMhpJvbll1+WzWbTyy+/rD///FNlypRR69at9c9//tNdpwAAAAA3spnb7Pv51NRU+fv7KyUlRX5+fu4uBwAAAFfITV6z1GoGAAAAwOUIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLLcHmbff/99hYaGysfHR+Hh4Vq/fv01+586dUr9+/dXuXLl5O3trapVq2rx4sUFVC0AAABuJoXcefC5c+cqNjZWkydPVnh4uMaPH6/IyEjt2rVLZcuWdemfnp6uhx56SGXLltX8+fMVHBysgwcPqkSJEgVfPAAAANzOZowx7jp4eHi47rvvPk2aNEmSZLfbFRISooEDB2rEiBEu/SdPnqy33npLO3fuVOHChW/omKmpqfL391dKSor8/Pz+Vv0AAADIe7nJa267zCA9PV0bN25URETE/4rx8FBERITWrVuX7T7ffPONGjZsqP79+ysgIEA1a9bU2LFjlZmZWVBlAwAA4CbitssMjh8/rszMTAUEBDi1BwQEaOfOndnus2/fPq1cuVLdunXT4sWLtWfPHvXr108ZGRmKi4vLdp+0tDSlpaU5HqempubdSQAAAMCt3H4DWG7Y7XaVLVtWU6ZMUVhYmKKjo/XSSy9p8uTJV90nPj5e/v7+ji0kJKQAKwYAAEB+cluYLV26tDw9PZWUlOTUnpSUpMDAwGz3KVeunKpWrSpPT09HW/Xq1ZWYmKj09PRs9xk5cqRSUlIc2+HDh/PuJAAAAOBWbguzXl5eCgsL04oVKxxtdrtdK1asUMOGDbPdp3HjxtqzZ4/sdruj7Y8//lC5cuXk5eWV7T7e3t7y8/Nz2gAAAHBrcOtlBrGxsfr44481Y8YM7dixQ3379tXZs2fVq1cvSVKPHj00cuRIR/++ffvqxIkTGjx4sP744w8tWrRIY8eOVf/+/d11CgAAAHAjt64zGx0dreTkZI0aNUqJiYmqU6eOli5d6rgp7NChQ/Lw+F/eDgkJ0bJlyzRkyBDde++9Cg4O1uDBg/XCCy+46xQAAADgRm5dZ9YdWGcWAADg5maJdWYBAACAv4swCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALKvQjew0f/58ffHFFzp06JDS09Odntu0aVOeFAYAAABcT65nZv/1r3+pV69eCggI0G+//aYGDRqoVKlS2rdvnx555JH8qBEAAADIVq5nZj/44ANNmTJFXbp00fTp0zV8+HBVqlRJo0aN0okTJ/KjRgDATc422ubuEgDkMxNn3F1CtnI9M3vo0CE1atRIkuTr66vTp09Lkrp3767PP/88b6sDAAAAriHXYTYwMNAxA1uhQgX98ssvkqT9+/fLmJszsQMAAODWlOsw26JFC33zzTeSpF69emnIkCF66KGHFB0drfbt2+d5gQAAAMDV5Pqa2SlTpshut0uS+vfvr1KlSunnn39WmzZt9Mwzz+R5gQAAAMDV5DrMenh4yMPjfxO6nTt3VufOnfO0KAAAACAnchRmt2zZopo1a8rDw0Nbtmy5Zt977703TwoDAAAAridHYbZOnTpKTExU2bJlVadOHdlstmxv9rLZbMrMzMzzIgEAAIDs5CjM7t+/X2XKlHH8NwAAAHAzyFGYrVixYrb/DQAAALhTrpfmio+P19SpU13ap06dqjfeeCNPigIAAAByIterGXz00UeaPXu2S/s999yjzp0764UXXsiTwm4lNv7KI3Bb4O/GAEDBy/XMbGJiosqVK+fSXqZMGR09ejRPigIAAAByItdhNiQkRGvXrnVpX7t2rYKCgvKkKAAAACAncn2ZQZ8+ffTcc88pIyNDLVq0kCStWLFCw4cP1/PPP5/nBQIAAABXk+swO2zYMP3111/q16+f0tPTJUk+Pj564YUXNHLkyDwvEAAAALgam8nurx/kwJkzZ7Rjxw75+vrqrrvukre3d17Xli9SU1Pl7++vlJQU+fn5FcgxuQEMuD3czjeA2UbzQQfc6kxcwX3I5Sav5XpmNkuxYsV033333ejuAAAAwN92Q2H2119/1RdffKFDhw45LjXIsmDBgjwpDAAAALie665msHr1ap0/f97xeM6cOWrcuLF27typefPmycvLS5s3b9YPP/ygEiVK5GetAAAAgJPrhtmdO3eqWbNmSk5OliSNHTtWEyZM0DfffCNjjObMmaNdu3apXbt2qlChQr4XDAAAAGS5bph9+umnNXDgQEVEREiS9u7dq5YtW0qSvLy8dO7cORUqVEjDhg3TRx99lL/VAgAAAJfJ0R9N6N69u+bPny9JKlmypE6fPi1JCg4O1tatWyVJJ0+e1Llz5/KpTAAAAMBVjv8C2F133SVJeuCBB7R8+XJJUqdOndSpUyc988wz6ty5sx566KH8qRIAAADIRq5XM5g0aZIuXLggSRozZoyKFSumX375RdHR0Xr55ZfzvEAAAADganIVZi9evKjvvvtOkZGRl3YuVEgvvfRSvhQGAAAAXE+OLzOQLoXXZ5991jEzCwAAALhTrsKsJDVo0EAJCQn5UAoAAACQO7m+ZrZfv36KjY3V4cOHFRYWpqJFizo9f++99+ZZcQAAAMC15DrMdu7cWZI0aNAgR5vNZpMxRjabTZmZmXlXHQAAAHANuQ6z+/fvz486AAAAgFzLdZitWLFiftQBAAAA5Fquw+ynn356zed79Ohxw8UAAAAAuZHrMDt48GCnxxkZGTp37py8vLxUpEgRwiwAAAAKTK6X5jp58qTTdubMGe3atUtNmjTR559/nh81AgAAANnKdZjNzl133aVx48a5zNoCAAAA+SlPwqx06a+DHTlyJK+GAwAAAK4r19fMfvPNN06PjTE6evSoJk2apMaNG+dZYQAAAMD15DrMtmvXzumxzWZTmTJl1KJFC73zzjt5VRcAAABwXbkOs3a7PT/qAAAAAHItz66ZBQAAAAparsNsx44d9cYbb7i0v/nmm3r88cfzpCgAAAAgJ3IdZlevXq1WrVq5tD/yyCNavXp1nhQFAAAA5ESuw+yZM2fk5eXl0l64cGGlpqbmSVEAAABATuQ6zNaqVUtz5851aZ8zZ45q1KiRJ0UBAAAAOZHr1QxeeeUVdejQQXv37lWLFi0kSStWrNDs2bM1f/78PC8QAAAAuJpch9nWrVtr4cKFGjt2rObPny9fX1/Vrl1bK1eu1B133JEfNQIAAADZynWYlaSoqChFRUVJklJTU/X5559r6NCh2rhxozIzM/O0QAAAAOBqbnid2dWrVysmJkZBQUF655131KJFC/3yyy95WRsAAABwTbmamU1MTNT06dP1ySefKDU1VZ06dVJaWpoWLlzIzV8AAAAocDmemW3durWqVaumLVu2aPz48Tpy5IgmTpyYn7UBAAAA15TjmdklS5Zo0KBB6tu3r+666678rAkAAADIkRzPzK5Zs0anT59WWFiYwsPDNWnSJB0/fjw/awMAAACuKcdh9v7779fHH3+so0eP6plnntGcOXMUFBQku92u5cuX6/Tp0/lZJwAAAOAi16sZFC1aVE8++aTWrFmjrVu36vnnn9e4ceNUtmxZtWnTJj9qBAAAALJ1w0tzSVK1atX05ptv6v/+7//0+eef51VNAAAAQI78rTCbxdPTU+3atdM333yTF8MBAAAAOZInYRYAAABwB8IsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALOumCLPvv/++QkND5ePjo/DwcK1fvz5H+82ZM0c2m03t2rXL3wIBAABwU3J7mJ07d65iY2MVFxenTZs2qXbt2oqMjNSxY8euud+BAwc0dOhQNW3atIAqBQAAwM3G7WH23XffVZ8+fdSrVy/VqFFDkydPVpEiRTR16tSr7pOZmalu3bpp9OjRqlSpUgFWCwAAgJuJW8Nsenq6Nm7cqIiICEebh4eHIiIitG7duqvu99prr6ls2bJ66qmnrnuMtLQ0paamOm0AAAC4Nbg1zB4/flyZmZkKCAhwag8ICFBiYmK2+6xZs0affPKJPv744xwdIz4+Xv7+/o4tJCTkb9cNAACAm4PbLzPIjdOnT6t79+76+OOPVbp06RztM3LkSKWkpDi2w4cP53OVAAAAKCiF3Hnw0qVLy9PTU0lJSU7tSUlJCgwMdOm/d+9eHThwQK1bt3a02e12SVKhQoW0a9cuVa5c2Wkfb29veXt750P1AAAAcDe3zsx6eXkpLCxMK1ascLTZ7XatWLFCDRs2dOl/9913a+vWrUpISHBsbdq0UfPmzZWQkMAlBAAAALcZt87MSlJsbKxiYmJUv359NWjQQOPHj9fZs2fVq1cvSVKPHj0UHBys+Ph4+fj4qGbNmk77lyhRQpJc2gEAAHDrc3uYjY6OVnJyskaNGqXExETVqVNHS5cuddwUdujQIXl4WOrSXgAAABQQmzHGuLuIgpSamip/f3+lpKTIz8+vQI5psxXIYQC42e31aerMNpoPOuBWZ+IK7kMuN3mNKU8AAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABY1k0RZt9//32FhobKx8dH4eHhWr9+/VX7fvzxx2ratKlKliypkiVLKiIi4pr9AQAAcOtye5idO3euYmNjFRcXp02bNql27dqKjIzUsWPHsu2/atUqdenSRT/88IPWrVunkJAQPfzww/rzzz8LuHIAAAC4m80YY9xZQHh4uO677z5NmjRJkmS32xUSEqKBAwdqxIgR190/MzNTJUuW1KRJk9SjR4/r9k9NTZW/v79SUlLk5+f3t+vPCZutQA4DwM3c+2nqXrbRfNABtzoTV3AfcrnJa26dmU1PT9fGjRsVERHhaPPw8FBERITWrVuXozHOnTunjIwM3XHHHdk+n5aWptTUVKcNAAAAtwa3htnjx48rMzNTAQEBTu0BAQFKTEzM0RgvvPCCgoKCnALx5eLj4+Xv7+/YQkJC/nbdAAAAuDm4/ZrZv2PcuHGaM2eOvvrqK/n4+GTbZ+TIkUpJSXFshw8fLuAqAQAAkF8KufPgpUuXlqenp5KSkpzak5KSFBgYeM193377bY0bN07ff/+97r333qv28/b2lre3d57UCwAAgJuLW2dmvby8FBYWphUrVjja7Ha7VqxYoYYNG151vzfffFNjxozR0qVLVb9+/YIoFQAAADcht87MSlJsbKxiYmJUv359NWjQQOPHj9fZs2fVq1cvSVKPHj0UHBys+Ph4SdIbb7yhUaNGafbs2QoNDXVcW1usWDEVK1bMbecBAACAguf2MBsdHa3k5GSNGjVKiYmJqlOnjpYuXeq4KezQoUPy8PjfBPKHH36o9PR0PfbYY07jxMXF6dVXXy3I0gEAAOBmbl9ntqCxziyA/HJ7fZo6Y51Z4NbHOrMAAABAHiPMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAy7opwuz777+v0NBQ+fj4KDw8XOvXr79m/3nz5unuu++Wj4+PatWqpcWLFxdQpQAAALiZuD3Mzp07V7GxsYqLi9OmTZtUu3ZtRUZG6tixY9n2//nnn9WlSxc99dRT+u2339SuXTu1a9dOv//+ewFXDgAAAHezGWOMOwsIDw/Xfffdp0mTJkmS7Ha7QkJCNHDgQI0YMcKlf3R0tM6ePavvvvvO0Xb//ferTp06mjx58nWPl5qaKn9/f6WkpMjPzy/vTuQabLYCOQwAN3Pvp6l72UbzQQfc6kxcwX3I5SavuXVmNj09XRs3blRERISjzcPDQxEREVq3bl22+6xbt86pvyRFRkZetT8AAABuXYXcefDjx48rMzNTAQEBTu0BAQHauXNntvskJiZm2z8xMTHb/mlpaUpLS3M8TklJkXQp8QNAXrqtP1YuuLsAAPmtILNT1rFycgGBW8NsQYiPj9fo0aNd2kNCQtxQDYBbmb+/uysAgPzjP67gP+ROnz4t/+t8uLo1zJYuXVqenp5KSkpyak9KSlJgYGC2+wQGBuaq/8iRIxUbG+t4bLfbdeLECZUqVUo2LmZFPklNTVVISIgOHz5cYNdmA0BB4TMO+c0Yo9OnTysoKOi6fd0aZr28vBQWFqYVK1aoXbt2ki6FzRUrVmjAgAHZ7tOwYUOtWLFCzz33nKNt+fLlatiwYbb9vb295e3t7dRWokSJvCgfuC4/Pz8+6AHcsviMQ3663oxsFrdfZhAbG6uYmBjVr19fDRo00Pjx43X27Fn16tVLktSjRw8FBwcrPj5ekjR48GA1a9ZM77zzjqKiojRnzhz9+uuvmjJlijtPAwAAAG7g9jAbHR2t5ORkjRo1SomJiapTp46WLl3quMnr0KFD8vD436ILjRo10uzZs/Xyyy/rxRdf1F133aWFCxeqZs2a7joFAAAAuInb15kFbkVpaWmKj4/XyJEjXS5zAQCr4zMONxPCLAAAACzL7X/OFgAAALhRhFkAAABYFmEWAAAAlkWYBfLR9OnTWdcYAIB8RJjFbatnz56y2Wx69tlnXZ7r37+/bDabevbsWfCFAUA+y/r8y9pKlSqlli1basuWLY4+lz9/+TZnzhxJ0qpVq5zay5Qpo1atWmnr1q3X3D9re/XVV91x6rgFEWZxWwsJCdGcOXN0/vx5R9uFCxc0e/ZsVahQ4W+NnZGR8XfLA4B807JlSx09elRHjx7VihUrVKhQIT366KNOfaZNm+bok7Vl/cXOLLt27dLRo0e1bNkypaWlKSoqSunp6U77jB8/Xn5+fk5tQ4cOLcCzxa2MMIvbWr169RQSEqIFCxY42hYsWKAKFSqobt26jralS5eqSZMmKlGihEqVKqVHH31Ue/fudTx/4MAB2Ww2zZ07V82aNZOPj49mzZrlcrzk5GTVr19f7du3V1pamtLS0jRo0CCVLVtWPj4+atKkiTZs2CDp0p92Ll++vD788EOnMX777Td5eHjo4MGDef1yALiNeHt7KzAwUIGBgapTp45GjBihw4cPKzk52dGnRIkSjj5Zm4+Pj9M4ZcuWVWBgoOrVq6fnnntOhw8f1s6dO5328ff3l81mc2orVqxYQZ8yblGEWdz2nnzySU2bNs3xeOrUqY4/p5zl7Nmzio2N1a+//qoVK1bIw8ND7du3l91ud+o3YsQIDR48WDt27FBkZKTTc4cPH1bTpk1Vs2ZNzZ8/X97e3ho+fLi+/PJLzZgxQ5s2bVKVKlUUGRmpEydOyMPDQ126dNHs2bOdxpk1a5YaN26sihUr5vErAeB2debMGX322WeqUqWKSpUqdUNjpKSkOC5B8PLyysvygGszwG0qJibGtG3b1hw7dsx4e3ubAwcOmAMHDhgfHx+TnJxs2rZta2JiYrLdNzk52UgyW7duNcYYs3//fiPJjB8/3qnftGnTjL+/v9m5c6cJCQkxgwYNMna73RhjzJkzZ0zhwoXNrFmzHP3T09NNUFCQefPNN40xxvz222/GZrOZgwcPGmOMyczMNMHBwebDDz/M65cDwG0kJibGeHp6mqJFi5qiRYsaSaZcuXJm48aNjj6SjI+Pj6NP1pb1efTDDz8YSU5jSDJt2rRxOV7WZyGQH5iZxW2vTJkyioqK0vTp0zVt2jRFRUWpdOnSTn12796tLl26qFKlSvLz81NoaKgk6dChQ0796tev7zL++fPn1bRpU3Xo0EETJkyQzWaTJO3du1cZGRlq3Lixo2/hwoXVoEED7dixQ5JUp04dVa9e3TE7++OPP+rYsWN6/PHH8+z8AdyemjdvroSEBCUkJGj9+vWKjIzUI4884nQJ03vvvefok7UFBQU5jfPTTz9p48aNmj59uqpWrarJkycX9KngNlfI3QUAN4Mnn3xSAwYMkCS9//77Ls+3bt1aFStW1Mcff6ygoCDZ7XbVrFlT6enpTv2KFi3qsq+3t7ciIiL03XffadiwYQoODs5Vbd26ddPs2bM1YsQIzZ49Wy1btrzhrwEBIEvRokVVpUoVx+N///vf8vf318cff6zXX39dkhQYGOjUJzt33nmnSpQooWrVqunYsWOKjo7W6tWr87V24HLMzAK6dFdvenq6MjIyXK51/euvv7Rr1y69/PLL+sc//qHq1avr5MmTOR7bw8NDM2fOVFhYmJo3b64jR45IkipXriwvLy+tXbvW0TcjI0MbNmxQjRo1HG1du3bV77//ro0bN2r+/Pnq1q3b3zxbAHBls9nk4eHhtLpLbvXv31+///67vvrqqzysDLg2ZmYBSZ6eno6v9j09PZ2eK1mypEqVKqUpU6aoXLlyOnTokEaMGJHr8WfNmqUuXbqoRYsWWrVqlQIDA9W3b18NGzZMd9xxhypUqKA333xT586d01NPPeXYNzQ0VI0aNdJTTz2lzMxMtWnT5u+fMIDbXlpamhITEyVJJ0+e1KRJk3TmzBm1bt3a0efUqVOOPlmKFy+e7bdQklSkSBH16dNHcXFxateuneOyKiA/MTML/H9+fn7y8/Nzaffw8NCcOXO0ceNG1axZU0OGDNFbb72V6/ELFSqkzz//XPfcc49atGihY8eOady4cerYsaO6d++uevXqac+ePVq2bJlKlizptG+3bt20efNmtW/fXr6+vjd8jgCQZenSpSpXrpzKlSun8PBwbdiwQfPmzdODDz7o6NOrVy9Hn6xt4sSJ1xx3wIAB2rFjh+bNm5fPZwBcYjPGGHcXAQAAANwIZmYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAi1q1apVsNptOnTqV431CQ0M1fvz4fKsJAAoaYRYA8knPnj1ls9n07LPPujzXv39/2Ww29ezZs+ALA4BbCGEWAPJRSEiI5syZo/PnzzvaLly4oNmzZ6tChQpurAwAbg2EWQDIR/Xq1VNISIgWLFjgaFuwYIEqVKigunXrOtrS0tI0aNAglS1bVj4+PmrSpIk2bNjgNNbixYtVtWpV+fr6qnnz5jpw4IDL8dasWaOmTZvK19dXISEhGjRokM6ePXvV+g4dOqS2bduqWLFi8vPzU6dOnZSUlOR4fvPmzWrevLmKFy8uPz8/hYWF6ddff/0brwgA5C3CLADksyeffFLTpk1zPJ46dap69erl1Gf48OH68ssvNWPGDG3atElVqlRRZGSkTpw4IUk6fPiwOnTooNatWyshIUG9e/fWiBEjnMbYu3evWrZsqY4dO2rLli2aO3eu1qxZowEDBmRbl91uV9u2bXXixAn9+OOPWr58ufbt26fo6GhHn27duql8+fLasGGDNm7cqBEjRqhw4cJ59dIAwN9nAAD5IiYmxrRt29YcO3bMeHt7mwMHDpgDBw4YHx8fk5ycbNq2bWtiYmLMmTNnTOHChc2sWbMc+6anp5ugoCDz5ptvGmOMGTlypKlRo4bT+C+88IKRZE6ePGmMMeapp54yTz/9tFOfn376yXh4eJjz588bY4ypWLGiee+994wxxvznP/8xnp6e5tChQ47+27ZtM5LM+vXrjTHGFC9e3EyfPj1PXxcAyEuF3B2mAeBWV6ZMGUVFRWn69OkyxigqKkqlS5d2PL93715lZGSocePGjrbChQurQYMG2rFjhyRpx44dCg8Pdxq3YcOGTo83b96sLVu2aNasWY42Y4zsdrv279+v6tWrO/XfsWOHQkJCFBIS4mirUaOGSpQooR07dui+++5TbGysevfurZkzZyoiIkKPP/64Kleu/PdfFADII1xmAAAF4Mknn9T06dM1Y8YMPfnkk/lyjDNnzuiZZ55RQkKCY9u8ebN27959wwH01Vdf1bZt2xQVFaWVK1eqRo0a+uqrr/K4cgC4cYRZACgALVu2VHp6ujIyMhQZGen0XOXKleXl5aW1a9c62jIyMrRhwwbVqFFDklS9enWtX7/eab9ffvnF6XG9evW0fft2ValSxWXz8vJyqal69eo6fPiwDh8+7Gjbvn27Tp065TiuJFWtWlVDhgzRf/7zH3Xo0MHp+l8AcDfCLAAUAE9PT+3YsUPbt2+Xp6en03NFixZV3759NWzYMC1dulTbt29Xnz59dO7cOT311FOSpGeffVa7d+/WsGHDtGvXLs2ePVvTp093GueFF17Qzz//rAEDBighIUG7d+/W119/fdUbwCIiIlSrVi1169ZNmzZt0vr169WjRw81a9ZM9evX1/nz5zVgwACtWrVKBw8e1Nq1a7VhwwaXyxUAwJ0IswBQQPz8/OTn55ftc+PGjVPHjh3VvXt31atXT3v27NGyZctUsmRJSVKFChX05ZdfauHChapdu7YmT56ssWPHOo1x77336scff9Qff/yhpk2bqm7duho1apSCgoKyPabNZtPXX3+tkiVL6oEHHlBERIQqVaqkuXPnSroUwP/66y/16NFDVatWVadOnfTII49o9OjRefiqAMDfYzPGGHcXAQAAANwIZmYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBl/T9RakTx7MN39gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Validation of the Markov Model\n",
        "\n",
        "### Description\n",
        "\n",
        "This cell performs cross-validation on the Markov model and plots the results to check for potential overfitting or underfitting issues.\n",
        "\n",
        "### **Cross-Validation:**\n",
        "- Uses `cross_val_score` to calculate accuracy across 5 folds of the training set.\n",
        "- Plots the accuracy for each fold in a line chart with markers.\n",
        "\n",
        "### **Chart:**\n",
        "- Displays the accuracy for each fold in the cross-validation process.\n",
        "- A horizontal line indicates the test set accuracy for comparison.\n",
        "- Includes a title, axis labels, and legend for clarity.\n",
        "\n",
        "### **Purpose:**\n",
        "- Assess the consistency of the Markov model across different subsets of data.\n",
        "- Detect overfitting (if test accuracy is much higher than fold accuracies) or underfitting (if all accuracies are low).\n",
        "\n",
        "This chart helps understand whether the model is generalizing well or overly fitted to the training data, offering a more robust view of its performance."
      ],
      "metadata": {
        "id": "1zU44ckmkJd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score  # Importa função para validação cruzada.\n",
        "\n",
        "# Realizar validação cruzada no modelo Markov\n",
        "cv_scores = cross_val_score(markov_classifier, X_train_markov, y_train, cv=5)  # Calcula acurácia em 5 folds.\n",
        "\n",
        "# Plotar os resultados da validação cruzada\n",
        "plt.figure(figsize=(8, 6))  # Define o tamanho da figura.\n",
        "plt.plot(range(1, 6), cv_scores, marker='o', linestyle='--', color='purple')  # Plota a acurácia de cada fold.\n",
        "plt.axhline(y=markov_accuracy, color='blue', linestyle='-', label=f'Acurácia Teste: {markov_accuracy:.4f}')  # Linha da acurácia no teste.\n",
        "plt.title('Validação Cruzada do Modelo Markov')  # Título do gráfico.\n",
        "plt.xlabel('Fold')  # Rótulo do eixo x (cada fold da validação cruzada).\n",
        "plt.ylabel('Acurácia')  # Rótulo do eixo y.\n",
        "plt.ylim(0, 1)  # Limite do eixo y entre 0 e 1.\n",
        "plt.legend()  # Exibe a legenda.\n",
        "plt.grid(True)  # Adiciona uma grade para facilitar a leitura.\n",
        "plt.show()  # Exibe o gráfico.\n",
        "\n",
        "# Exibir média e desvio padrão da validação cruzada\n",
        "print(f\"Média da acurácia na validação cruzada: {cv_scores.mean():.4f}\")\n",
        "print(f\"Desvio padrão da acurácia na validação cruzada: {cv_scores.std():.4f}\")"
      ],
      "metadata": {
        "id": "x64JWoijkJxu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "412be0d2-264c-43b9-8bad-3b3125c64841"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Média da acurácia na validação cruzada: 0.5300\n",
            "Desvio padrão da acurácia na validação cruzada: 0.0042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qmflE1jGsALe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "masked_language_modeling",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}